

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="2.2 输入部分实现 P4 学习目标  了解文本嵌入层和位置编码的作用.  掌握文本嵌入层和位置编码的实现过程.       输入部分包含: 源文本嵌入层及其位置编码器 目标文本嵌入层及其位置编码器     文本嵌入层的作用 无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.    pytorch 0.3.0及其必备工具包的安装">
<meta property="og:type" content="article">
<meta property="og:title" content="第二章 Transformer">
<meta property="og:url" content="http://example.com/2023/11/08/Transformer/Transformer%E7%AC%AC%E4%BA%8C%E7%AB%A0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="2.2 输入部分实现 P4 学习目标  了解文本嵌入层和位置编码的作用.  掌握文本嵌入层和位置编码的实现过程.       输入部分包含: 源文本嵌入层及其位置编码器 目标文本嵌入层及其位置编码器     文本嵌入层的作用 无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.    pytorch 0.3.0及其必备工具包的安装">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/image-20231108193413280.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-0c210867908a13e1f477e491ec5a2062_720w.webp">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-306c89830ea9efe5953ae4d8b76492a4_1440w.webp">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-7e76569222acd94a74de6d2ba5efa824_1440w.webp">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-d7913da8a83d6ca9d67524560b263688_1440w.webp">
<meta property="og:image" content="https://article.biliimg.com/bfs/article/af87236e673d1a082c9941b51678674b155712160.png">
<meta property="og:image" content="https://article.biliimg.com/bfs/article/7e3ca1eb7eb850ebfa19b2b4039ddd59155712160.png">
<meta property="og:image" content="https://article.biliimg.com/bfs/article/122092d91f732d6ea7ef61d100d3f1fc155712160.png">
<meta property="og:image" content="https://article.biliimg.com/bfs/article/442c5ecb840fdeae191a0184e836f395155712160.png">
<meta property="og:image" content="https://article.biliimg.com/bfs/article/f46db8ec8cc3f5cc82fe1a014104a212155712160.png">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-306c89830ea9efe5953ae4d8b76492a4_1440w.webp">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-4e17a521400c92b41a5927e5c40aeef0_1440w.webp">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-4b2d21a7199c6f261c9caca82e213b96_1440w.webp">
<meta property="og:image" content="https://article.biliimg.com/bfs/article/e29cac3e97a731b009805f5fc7842ffa155712160.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202311052258109.png">
<meta property="article:published_time" content="2023-11-08T12:09:25.415Z">
<meta property="article:modified_time" content="2023-11-08T12:12:24.179Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/image-20231108193413280.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>第二章 Transformer - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.4","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":false,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  



  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 60vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>wbupt</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">第二章 Transformer</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-11-08 20:09" pubdate>
          2023年11月8日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          58k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          482 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">第二章 Transformer</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：6 个月前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h2 id="2-2-输入部分实现-P4"><a href="#2-2-输入部分实现-P4" class="headerlink" title="2.2 输入部分实现 P4"></a>2.2 输入部分实现 P4</h2><ul>
<li><p>学习目标</p>
<ul>
<li><p>了解文本嵌入层和位置编码的作用.</p>
</li>
<li><p>掌握文本嵌入层和位置编码的实现过程.</p>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>输入部分包含:<ul>
<li>源文本嵌入层及其位置编码器</li>
<li>目标文本嵌入层及其位置编码器</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/image-20231108193413280.png" srcset="/img/loading.gif" lazyload alt="image-20231108193413280"></p>
<h3 id="文本嵌入层的作用"><a href="#文本嵌入层的作用" class="headerlink" title="文本嵌入层的作用"></a>文本嵌入层的作用</h3><ul>
<li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li>
</ul>
<hr>
<ul>
<li><p>pytorch 0.3.0及其必备工具包的安装:</p>
<h3 id="安装版本"><a href="#安装版本" class="headerlink" title="安装版本"></a>安装版本</h3></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用pip安装的工具包包括pytorch-0.3.0, numpy, matplotlib, seaborn</span><br>pip install http://download.pytorch.org/whl/cu80/torch-<span class="hljs-number">0.3</span><span class="hljs-number">.0</span>.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib seaborn<br><br><span class="hljs-comment"># MAC系统安装, python版本&lt;=3.6</span><br>pip install torch==<span class="hljs-number">0.3</span><span class="hljs-number">.0</span>.post4 numpy matplotlib seaborn<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">conda create -n postcoding python=<span class="hljs-number">3.0</span><br>conda activate postcoding<br><br>pip3 install torch==<span class="hljs-number">1.10</span><span class="hljs-number">.0</span>+cu113 torchvision==<span class="hljs-number">0.11</span><span class="hljs-number">.1</span>+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -i https://pypi.douban.com/simple<br>pip install xlrd==<span class="hljs-number">1.2</span><span class="hljs-number">.0</span><br>pip install xlrd==<span class="hljs-number">4.64</span><span class="hljs-number">.1</span><br>pip install matplotlib==<span class="hljs-number">3.2</span><span class="hljs-number">.0</span> -i https://pypi.douban.com/simple/<br></code></pre></td></tr></table></figure>

<ul>
<li>文本嵌入层的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入必备的工具包</span><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 预定义的网络层torch.nn, 工具开发者已经帮助我们开发好的一些常用层, </span><br><span class="hljs-comment"># 比如，卷积层, lstm层, embedding层等, 不需要我们再重新造轮子.</span><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-comment"># 数学计算工具包</span><br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># torch中变量封装函数Variable.</span><br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<br><br><span class="hljs-comment"># 定义Embeddings类来实现文本嵌入层，这里s说明代表两个一模一样的嵌入层, 他们共享参数.</span><br><span class="hljs-comment"># 该类继承nn.Module, 这样就有标准层的一些功能, 这里我们也可以理解为一种模式, 我们自己实现的所有层都会这样去写.</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Embeddings</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;类的初始化函数, 有两个参数, d_model: 指词嵌入的维度, vocab: 指词表的大小.&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 接着就是使用super的方式指明继承nn.Module的初始化函数, 我们自己实现的所有层都会这样去写.</span><br>        <span class="hljs-built_in">super</span>(Embeddings, self).__init__()<br>        <span class="hljs-comment"># 之后就是调用nn中的预定义层Embedding, 获得一个词嵌入对象self.lut</span><br>        self.lut = nn.Embedding(vocab, d_model)<br>        <span class="hljs-comment"># 最后就是将d_model传入类中</span><br>        self.d_model = d_model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;可以将其理解为该层的前向传播逻辑，所有层中都会有此函数</span><br><span class="hljs-string">           当传给该类的实例化对象参数时, 自动调用该类函数</span><br><span class="hljs-string">           参数x: 因为Embedding层是首层, 所以代表输入给模型的文本通过词汇映射后的张量&quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 将x传给self.lut并与根号下self.d_model相乘作为结果返回</span><br>        <span class="hljs-keyword">return</span> self.lut(x) * math.sqrt(self.d_model)	<span class="hljs-comment"># sqrt(self.d_model)缩放作用</span><br></code></pre></td></tr></table></figure>

<p><code>P5</code></p>
<ul>
<li>nn.Embedding演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>embedding = nn.Embedding(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = torch.LongTensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">9</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>embedding(<span class="hljs-built_in">input</span>)<br>tensor([[[-<span class="hljs-number">0.0251</span>, -<span class="hljs-number">1.6902</span>,  <span class="hljs-number">0.7172</span>],<br>         [-<span class="hljs-number">0.6431</span>,  <span class="hljs-number">0.0748</span>,  <span class="hljs-number">0.6969</span>],<br>         [ <span class="hljs-number">1.4970</span>,  <span class="hljs-number">1.3448</span>, -<span class="hljs-number">0.9685</span>],<br>         [-<span class="hljs-number">0.3677</span>, -<span class="hljs-number">2.7265</span>, -<span class="hljs-number">0.1685</span>]],<br><br>        [[ <span class="hljs-number">1.4970</span>,  <span class="hljs-number">1.3448</span>, -<span class="hljs-number">0.9685</span>],<br>         [ <span class="hljs-number">0.4362</span>, -<span class="hljs-number">0.4004</span>,  <span class="hljs-number">0.9400</span>],<br>         [-<span class="hljs-number">0.6431</span>,  <span class="hljs-number">0.0748</span>,  <span class="hljs-number">0.6969</span>],<br>         [ <span class="hljs-number">0.9124</span>, -<span class="hljs-number">2.3616</span>,  <span class="hljs-number">1.1151</span>]]])<br><br><br><span class="hljs-meta">&gt;&gt;&gt; </span>embedding = nn.Embedding(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, padding_idx=<span class="hljs-number">0</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = torch.LongTensor([[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">5</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>embedding(<span class="hljs-built_in">input</span>)<br>tensor([[[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],<br>         [ <span class="hljs-number">0.1535</span>, -<span class="hljs-number">2.0309</span>,  <span class="hljs-number">0.9315</span>],<br>         [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],<br>         [-<span class="hljs-number">0.1655</span>,  <span class="hljs-number">0.9897</span>,  <span class="hljs-number">0.0635</span>]]])<br></code></pre></td></tr></table></figure>

<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 词嵌入维度是512维</span><br>d_model = <span class="hljs-number">512</span><br><br><span class="hljs-comment"># 词表大小是1000</span><br>vocab = <span class="hljs-number">1000</span><br></code></pre></td></tr></table></figure>

<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入x是一个使用Variable封装的长整型张量, 形状是2 x 4</span><br>x = Variable(torch.LongTensor([[<span class="hljs-number">100</span>,<span class="hljs-number">2</span>,<span class="hljs-number">421</span>,<span class="hljs-number">508</span>],[<span class="hljs-number">491</span>,<span class="hljs-number">998</span>,<span class="hljs-number">1</span>,<span class="hljs-number">221</span>]]))<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">emb = Embeddings(d_model, vocab)<br>embr = emb(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;embr:&quot;</span>, embr)<br></code></pre></td></tr></table></figure>

<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">embr: Variable containing:<br>( <span class="hljs-number">0</span> ,.,.) = <br>  <span class="hljs-number">35.9321</span>   <span class="hljs-number">3.2582</span> -<span class="hljs-number">17.7301</span>  ...    <span class="hljs-number">3.4109</span>  <span class="hljs-number">13.8832</span>  <span class="hljs-number">39.0272</span><br>   <span class="hljs-number">8.5410</span>  -<span class="hljs-number">3.5790</span> -<span class="hljs-number">12.0460</span>  ...   <span class="hljs-number">40.1880</span>  <span class="hljs-number">36.6009</span>  <span class="hljs-number">34.7141</span><br> -<span class="hljs-number">17.0650</span>  -<span class="hljs-number">1.8705</span> -<span class="hljs-number">20.1807</span>  ...  -<span class="hljs-number">12.5556</span> -<span class="hljs-number">34.0739</span>  <span class="hljs-number">35.6536</span><br>  <span class="hljs-number">20.6105</span>   <span class="hljs-number">4.4314</span>  <span class="hljs-number">14.9912</span>  ...   -<span class="hljs-number">0.1342</span>  -<span class="hljs-number">9.9270</span>  <span class="hljs-number">28.6771</span><br><br>( <span class="hljs-number">1</span> ,.,.) = <br>  <span class="hljs-number">27.7016</span>  <span class="hljs-number">16.7183</span>  <span class="hljs-number">46.6900</span>  ...   <span class="hljs-number">17.9840</span>  <span class="hljs-number">17.2525</span>  -<span class="hljs-number">3.9709</span><br>   <span class="hljs-number">3.0645</span>  -<span class="hljs-number">5.5105</span>  <span class="hljs-number">10.8802</span>  ...  -<span class="hljs-number">13.0069</span>  <span class="hljs-number">30.8834</span> -<span class="hljs-number">38.3209</span><br>  <span class="hljs-number">33.1378</span> -<span class="hljs-number">32.1435</span>  -<span class="hljs-number">3.9369</span>  ...   <span class="hljs-number">15.6094</span> -<span class="hljs-number">29.7063</span>  <span class="hljs-number">40.1361</span><br> -<span class="hljs-number">31.5056</span>   <span class="hljs-number">3.3648</span>   <span class="hljs-number">1.4726</span>  ...    <span class="hljs-number">2.8047</span>  -<span class="hljs-number">9.6514</span> -<span class="hljs-number">23.4909</span><br>[torch.FloatTensor of size 2x4x512]<br></code></pre></td></tr></table></figure>

<ul>
<li>输出效果</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> copy<br><br><br><span class="hljs-comment"># embedding = nn.Embedding(10, 3)</span><br><span class="hljs-comment"># input1 = torch.LongTensor([[1,2,4,5],[4,3,2,9]])</span><br><span class="hljs-comment"># embedding = nn.Embedding(10, 3)</span><br><span class="hljs-comment"># input1 = torch.LongTensor([[1 ,2, 4, 5], [4, 3, 2, 9]])</span><br><span class="hljs-comment"># print(embedding(input1))</span><br><span class="hljs-comment"># embedding = nn.Embedding(10, 3, padding_idx=0)</span><br><span class="hljs-comment"># input1 = torch.LongTensor([[0, 2, 0, 5]])</span><br><span class="hljs-comment"># print(embedding(input1))</span><br><span class="hljs-comment"># 构建Embedding类来实现文本嵌入层</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Embeddings</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, vocab</span>):<br>        <span class="hljs-comment"># d_model: 词嵌入的维度</span><br>        <span class="hljs-comment"># vocab: 词表的大小</span><br>        <span class="hljs-built_in">super</span>(Embeddings, self).__init__()<br>        <span class="hljs-comment"># 定义Embedding层</span><br>        self.lut = nn.Embedding(vocab, d_model)<br>        <span class="hljs-comment"># 将参数传入到类中</span><br>        self.d_model = d_model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x：代表输入进模型的文本通过词汇映射后的数字张量</span><br>        <span class="hljs-keyword">return</span> self.lut(x) * math.sqrt(self.d_model)<br><br><br><span class="hljs-comment"># 实例化参数</span><br>d_model = <span class="hljs-number">512</span><br>vocab = <span class="hljs-number">1000</span><br><br>x = Variable(torch.LongTensor([[<span class="hljs-number">100</span>, <span class="hljs-number">2</span>, <span class="hljs-number">421</span>, <span class="hljs-number">508</span>], [<span class="hljs-number">491</span>, <span class="hljs-number">998</span>, <span class="hljs-number">1</span>, <span class="hljs-number">221</span>]]))<br><br>emb = Embeddings(d_model, vocab)<br>embr = emb(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;embr:&quot;</span>, embr)<br><span class="hljs-built_in">print</span>(embr.shape)<br></code></pre></td></tr></table></figure>

<p><code>P 6 位置编码器的作用</code></p>
<p>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失</p>
<ul>
<li>位置编码器的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义位置编码器类, 我们同样把它看做一个层, 因此会继承nn.Module    </span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionalEncoding</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, dropout, max_len=<span class="hljs-number">5000</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;位置编码器类的初始化函数, 共有三个参数, 分别是d_model: 词嵌入维度, </span><br><span class="hljs-string">           dropout: 置0比率, max_len: 每个句子的最大长度&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(PositionalEncoding, self).__init__()<br><br>        <span class="hljs-comment"># 实例化nn中预定义的Dropout层, 并将dropout传入其中, 获得对象self.dropout</span><br>        self.dropout = nn.Dropout(p=dropout)<br><br>        <span class="hljs-comment"># 初始化一个位置编码矩阵, 它是一个0阵，矩阵的大小是max_len x d_model.</span><br>        pe = torch.zeros(max_len, d_model)	<span class="hljs-comment"># 行 和 列</span><br><br>        <span class="hljs-comment"># 初始化一个绝对位置矩阵, 在我们这里，词汇的绝对位置就是用它的索引去表示. </span><br>        <span class="hljs-comment"># 所以我们首先使用arange方法获得一个连续自然数向量，然后再使用unsqueeze方法拓展向量维度使其成为矩阵， </span><br>        <span class="hljs-comment"># 又因为参数传的是1，代表矩阵拓展的位置，会使向量变成一个max_len x 1 的矩阵， </span><br>        position = torch.arange(<span class="hljs-number">0</span>, max_len).unsqueeze(<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># 绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中，</span><br>        <span class="hljs-comment"># 最简单思路就是先将max_len x 1的绝对位置矩阵， 变换成max_len x d_model形状，然后覆盖原来的初始位置编码矩阵即可， </span><br>        <span class="hljs-comment"># 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要求除了形状外，</span><br>        <span class="hljs-comment"># 还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛.  这样我们就可以开始初始化这个变换矩阵了.</span><br>        <span class="hljs-comment"># 首先使用arange获得一个自然数矩阵， 但是细心的同学们会发现， 我们这里并没有按照预计的一样初始化一个1xd_model的矩阵， </span><br>        <span class="hljs-comment"># 而是有了一个跳跃，只初始化了一半即1xd_model/2 的矩阵。 为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵，</span><br>        <span class="hljs-comment"># 我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正弦波上， 第二次初始化的变换矩阵分布在余弦波上， </span><br>        <span class="hljs-comment"># 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵.</span><br>        div_term = torch.exp(torch.arange(<span class="hljs-number">0</span>, d_model, <span class="hljs-number">2</span>) *<br>                             -(math.log(<span class="hljs-number">10000.0</span>) / d_model))<br>        pe[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(position * div_term)	<span class="hljs-comment"># 偶数列</span><br>        pe[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(position * div_term)<br><br>        <span class="hljs-comment"># 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，</span><br>        <span class="hljs-comment"># 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.</span><br>        pe = pe.unsqueeze(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，</span><br>        <span class="hljs-comment"># 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤进行更新的增益对象. </span><br>        <span class="hljs-comment"># 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.</span><br>        self.register_buffer(<span class="hljs-string">&#x27;pe&#x27;</span>, pe)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;forward函数的参数是x, 表示文本序列的词嵌入表示&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切片到与输入的x的第二维相同即x.size(1)，</span><br>        <span class="hljs-comment"># 因为我们默认max_len为5000一般来讲实在太大了，很难有一条句子包含5000个词汇，所以要进行与输入张量的适配. </span><br>        <span class="hljs-comment"># 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requires_grad设置成false.</span><br>        x = x + Variable(self.pe[:, :x.size(<span class="hljs-number">1</span>)], <br>                         requires_grad=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># 最后使用self.dropout对象进行&#x27;丢弃&#x27;操作, 并返回结果.</span><br>        <span class="hljs-keyword">return</span> self.dropout(x)<br></code></pre></td></tr></table></figure>

<ul>
<li>nn.Dropout演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Dropout(p=<span class="hljs-number">0.2</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>output = m(<span class="hljs-built_in">input</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>output<br>Variable containing:<br> <span class="hljs-number">0.0000</span> -<span class="hljs-number">0.5856</span> -<span class="hljs-number">1.4094</span>  <span class="hljs-number">0.0000</span> -<span class="hljs-number">1.0290</span><br> <span class="hljs-number">2.0591</span> -<span class="hljs-number">1.3400</span> -<span class="hljs-number">1.7247</span> -<span class="hljs-number">0.9885</span>  <span class="hljs-number">0.1286</span><br> <span class="hljs-number">0.5099</span>  <span class="hljs-number">1.3715</span>  <span class="hljs-number">0.0000</span>  <span class="hljs-number">2.2079</span> -<span class="hljs-number">0.5497</span><br>-<span class="hljs-number">0.0000</span> -<span class="hljs-number">0.7839</span> -<span class="hljs-number">1.2434</span> -<span class="hljs-number">0.1222</span>  <span class="hljs-number">1.2815</span><br>[torch.FloatTensor of size 4x5]<br></code></pre></td></tr></table></figure>

<ul>
<li>torch.unsqueeze演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="hljs-number">0</span>)<br>tensor([[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="hljs-number">1</span>)<br>tensor([[ <span class="hljs-number">1</span>],<br>        [ <span class="hljs-number">2</span>],<br>        [ <span class="hljs-number">3</span>],<br>        [ <span class="hljs-number">4</span>]])<br></code></pre></td></tr></table></figure>

<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 词嵌入维度是512维</span><br>d_model = <span class="hljs-number">512</span><br><br><span class="hljs-comment"># 置0比率为0.1</span><br>dropout = <span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># 句子最大长度</span><br>max_len=<span class="hljs-number">60</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入x是Embedding层的输出的张量, 形状是2 x 4 x 512</span><br>x = embr<br>Variable containing:<br>( <span class="hljs-number">0</span> ,.,.) = <br>  <span class="hljs-number">35.9321</span>   <span class="hljs-number">3.2582</span> -<span class="hljs-number">17.7301</span>  ...    <span class="hljs-number">3.4109</span>  <span class="hljs-number">13.8832</span>  <span class="hljs-number">39.0272</span><br>   <span class="hljs-number">8.5410</span>  -<span class="hljs-number">3.5790</span> -<span class="hljs-number">12.0460</span>  ...   <span class="hljs-number">40.1880</span>  <span class="hljs-number">36.6009</span>  <span class="hljs-number">34.7141</span><br> -<span class="hljs-number">17.0650</span>  -<span class="hljs-number">1.8705</span> -<span class="hljs-number">20.1807</span>  ...  -<span class="hljs-number">12.5556</span> -<span class="hljs-number">34.0739</span>  <span class="hljs-number">35.6536</span><br>  <span class="hljs-number">20.6105</span>   <span class="hljs-number">4.4314</span>  <span class="hljs-number">14.9912</span>  ...   -<span class="hljs-number">0.1342</span>  -<span class="hljs-number">9.9270</span>  <span class="hljs-number">28.6771</span><br><br>( <span class="hljs-number">1</span> ,.,.) = <br>  <span class="hljs-number">27.7016</span>  <span class="hljs-number">16.7183</span>  <span class="hljs-number">46.6900</span>  ...   <span class="hljs-number">17.9840</span>  <span class="hljs-number">17.2525</span>  -<span class="hljs-number">3.9709</span><br>   <span class="hljs-number">3.0645</span>  -<span class="hljs-number">5.5105</span>  <span class="hljs-number">10.8802</span>  ...  -<span class="hljs-number">13.0069</span>  <span class="hljs-number">30.8834</span> -<span class="hljs-number">38.3209</span><br>  <span class="hljs-number">33.1378</span> -<span class="hljs-number">32.1435</span>  -<span class="hljs-number">3.9369</span>  ...   <span class="hljs-number">15.6094</span> -<span class="hljs-number">29.7063</span>  <span class="hljs-number">40.1361</span><br> -<span class="hljs-number">31.5056</span>   <span class="hljs-number">3.3648</span>   <span class="hljs-number">1.4726</span>  ...    <span class="hljs-number">2.8047</span>  -<span class="hljs-number">9.6514</span> -<span class="hljs-number">23.4909</span><br>[torch.FloatTensor of size 2x4x512]<br></code></pre></td></tr></table></figure>

<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">pe = PositionalEncoding(d_model, dropout, max_len)<br>pe_result = pe(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;pe_result:&quot;</span>, pe_result)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">pe_result: Variable containing:<br>( <span class="hljs-number">0</span> ,.,.) = <br> -<span class="hljs-number">19.7050</span>   <span class="hljs-number">0.0000</span>   <span class="hljs-number">0.0000</span>  ...  -<span class="hljs-number">11.7557</span>  -<span class="hljs-number">0.0000</span>  <span class="hljs-number">23.4553</span><br>  -<span class="hljs-number">1.4668</span> -<span class="hljs-number">62.2510</span>  -<span class="hljs-number">2.4012</span>  ...   <span class="hljs-number">66.5860</span> -<span class="hljs-number">24.4578</span> -<span class="hljs-number">37.7469</span><br>   <span class="hljs-number">9.8642</span> -<span class="hljs-number">41.6497</span> -<span class="hljs-number">11.4968</span>  ...  -<span class="hljs-number">21.1293</span> -<span class="hljs-number">42.0945</span>  <span class="hljs-number">50.7943</span><br>   <span class="hljs-number">0.0000</span>  <span class="hljs-number">34.1785</span> -<span class="hljs-number">33.0712</span>  ...   <span class="hljs-number">48.5520</span>   <span class="hljs-number">3.2540</span>  <span class="hljs-number">54.1348</span><br><br>( <span class="hljs-number">1</span> ,.,.) = <br>   <span class="hljs-number">7.7598</span> -<span class="hljs-number">21.0359</span>  <span class="hljs-number">15.0595</span>  ...  -<span class="hljs-number">35.6061</span>  -<span class="hljs-number">0.0000</span>   <span class="hljs-number">4.1772</span><br> -<span class="hljs-number">38.7230</span>   <span class="hljs-number">8.6578</span>  <span class="hljs-number">34.2935</span>  ...  -<span class="hljs-number">43.3556</span>  <span class="hljs-number">26.6052</span>   <span class="hljs-number">4.3084</span><br>  <span class="hljs-number">24.6962</span>  <span class="hljs-number">37.3626</span> -<span class="hljs-number">26.9271</span>  ...   <span class="hljs-number">49.8989</span>   <span class="hljs-number">0.0000</span>  <span class="hljs-number">44.9158</span><br> -<span class="hljs-number">28.8435</span> -<span class="hljs-number">48.5963</span>  -<span class="hljs-number">0.9892</span>  ...  -<span class="hljs-number">52.5447</span>  -<span class="hljs-number">4.1475</span>  -<span class="hljs-number">3.0450</span><br>[torch.FloatTensor of size 2x4x512]<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>绘制词汇向量中特征的分布曲线:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 创建一张15 x 5大小的画布</span><br>plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">5</span>))<br><br><span class="hljs-comment"># 实例化PositionalEncoding类得到pe对象, 输入参数是20和0</span><br>pe = PositionalEncoding(<span class="hljs-number">20</span>, <span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数, </span><br><span class="hljs-comment"># 且这个tensor里的数值都是0, 被处理后相当于位置编码张量</span><br>y = pe(Variable(torch.zeros(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>, <span class="hljs-number">20</span>)))<br><br><span class="hljs-comment"># 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值</span><br><span class="hljs-comment"># 因为总共有20维之多, 我们这里只查看4，5，6，7维的值.</span><br>plt.plot(np.arange(<span class="hljs-number">100</span>), y[<span class="hljs-number">0</span>, :, <span class="hljs-number">4</span>:<span class="hljs-number">8</span>].data.numpy())<br><br><span class="hljs-comment"># 在画布上填写维度提示信息</span><br>plt.legend([<span class="hljs-string">&quot;dim %d&quot;</span>%p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>]])<br></code></pre></td></tr></table></figure>

<ul>
<li>输出效果:</li>
</ul>
<p><img src="https://pic3.zhimg.com/80/v2-0c210867908a13e1f477e491ec5a2062_720w.webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>效果分析:</p>
<ul>
<li><p>每条颜色的曲线代表某一个词汇中的特征在不同位置的含义.</p>
</li>
<li><p>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</p>
</li>
<li><p>正弦波和余弦波的值域范围都是1到-1这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.</p>
</li>
</ul>
<p><code>小节总结</code> p8</p>
<ul>
<li>学习了文本嵌入层的作用:<ul>
<li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了文本嵌入层的类: Embeddings<ul>
<li>初始化函数以d_model, 词嵌入维度, 和vocab, 词汇总数为参数, 内部主要使用了nn中的预定层Embedding进行词嵌入.</li>
<li>在forward函数中, 将输入x传入到Embedding的实例化对象中, 然后乘以一个根号下d_model进行缩放, 控制数值大小.</li>
<li>它的输出是文本嵌入后的结果.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了位置编码器的作用:<ul>
<li>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了位置编码器的类: PositionalEncoding<ul>
<li>初始化函数以d_model, dropout, max_len为参数, 分别代表d_model: 词嵌入维度, dropout: 置0比率, max_len: 每个句子的最大长度.</li>
<li>forward函数中的输入参数为x, 是Embedding层的输出.</li>
<li>最终输出一个加入了位置编码信息的词嵌入张量.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>实现了绘制词汇向量中特征的分布曲线:<ul>
<li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</li>
<li>正弦波和余弦波的值域范围都是1到-1, 这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.</li>
</ul>
</li>
</ul>
<h2 id="2-3-编码器部分实现-P10"><a href="#2-3-编码器部分实现-P10" class="headerlink" title="2.3 编码器部分实现 P10"></a>2.3 编码器部分实现 P10</h2><ul>
<li><p>学习目标</p>
<ul>
<li><p>了解编码器中各个组成部分的作用.</p>
</li>
<li><p>掌握编码器中各个组成部分的实现过程.</p>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器部分:<ul>
<li>由N个编码器层堆叠而成</li>
<li>每个编码器层由两个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-306c89830ea9efe5953ae4d8b76492a4_1440w.webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<ul>
<li>什么是掩码张量:<ul>
<li>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有<code>1</code>和<code>0</code>的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是<code>让另外一个张量中的一些数值被遮掩</code>，也可以说被替换, 它的表现形式是一个张量.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>掩码张量的作用:<ul>
<li>在transformer中, 掩码张量的主要作用在应用<code>attention</code>(将在下一小节讲解)时，有一些生成的attention张量中的值计算有可能已知了未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>生成掩码张量的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">subsequent_mask</span>(<span class="hljs-params">size</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;生成向后遮掩的掩码张量, 参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 在函数中, 首先定义掩码张量的形状</span><br>    attn_shape = (<span class="hljs-number">1</span>, size, size)<br><br>    <span class="hljs-comment"># 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间, </span><br>    <span class="hljs-comment"># 再使其中的数据类型变为无符号8位整形unit8 </span><br>    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="hljs-number">1</span>).astype(<span class="hljs-string">&#x27;uint8&#x27;</span>)<br><br>    <span class="hljs-comment"># 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作, </span><br>    <span class="hljs-comment"># 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减, </span><br>    <span class="hljs-comment"># 如果是0, subsequent_mask中的该位置由0变成1</span><br>    <span class="hljs-comment"># 如果是1, subsequent_mask中的该位置由1变成0 </span><br>    <span class="hljs-keyword">return</span> torch.from_numpy(<span class="hljs-number">1</span> - subsequent_mask)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>np.triu演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>np.triu([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>],[<span class="hljs-number">10</span>,<span class="hljs-number">11</span>,<span class="hljs-number">12</span>]], k=-<span class="hljs-number">1</span>)<br>array([[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],<br>       [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>],<br>       [ <span class="hljs-number">0</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>],<br>       [ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>, <span class="hljs-number">12</span>]])<br><br><br><span class="hljs-meta">&gt;&gt;&gt; </span>np.triu([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>],[<span class="hljs-number">10</span>,<span class="hljs-number">11</span>,<span class="hljs-number">12</span>]], k=<span class="hljs-number">0</span>)<br>array([[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],<br>       [ <span class="hljs-number">0</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>],<br>       [ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">9</span>],<br>       [ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br><br><br><span class="hljs-meta">&gt;&gt;&gt; </span>np.triu([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>],[<span class="hljs-number">10</span>,<span class="hljs-number">11</span>,<span class="hljs-number">12</span>]], k=<span class="hljs-number">1</span>)<br>array([[ <span class="hljs-number">0</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],<br>       [ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">6</span>],<br>       [ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>],<br>       [ <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入实例:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 生成的掩码张量的最后两维的大小</span><br>size = <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure>

<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sm = subsequent_mask(size)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;sm:&quot;</span>, sm)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 最后两维形成一个下三角阵</span><br>sm: (<span class="hljs-number">0</span> ,.,.) = <br>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span><br>[torch.ByteTensor of size 1x5x5]<br></code></pre></td></tr></table></figure>

<hr>
<p><code>P 11 </code></p>
<ul>
<li>掩码张量的可视化:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))<br>plt.imshow(subsequent_mask(<span class="hljs-number">20</span>)[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-7e76569222acd94a74de6d2ba5efa824_1440w.webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>效果分析:</p>
<ul>
<li>通过观察可视化方阵, 黄色是1的部分, 这里代表被遮掩, 紫色代表没有被遮掩的信息, 横坐标代表目标词汇的位置, 纵坐标代表可查看的位置;</li>
<li>我们看到, 在0的位置我们一看望过去都是黄色的, 都被遮住了，1的位置一眼望过去还是黄色, 说明第一次词还没有产生, 从第二个位置看过去, 就能看到位置1的词, 其他位置看不到, 以此类推.</li>
</ul>
<p><code>总结</code></p>
<ul>
<li>学习了什么是掩码张量:<ul>
<li>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩, 也可以说被替换, 它的表现形式是一个张量.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了掩码张量的作用:<ul>
<li>在transformer中, 掩码张量的主要作用在应用attention(将在下一小节讲解)时，有一些生成的attetion张量中的值计算有可能已知量未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了生成向后遮掩的掩码张量函数: subsequent_mask<ul>
<li>它的输入是size, 代表掩码张量的大小.</li>
<li>它的输出是一个最后两维形成1方阵的下三角阵.</li>
<li>最后对生成的掩码张量进行了可视化分析, 更深一步理解了它的用途.</li>
</ul>
</li>
</ul>
<h3 id="2-3-2-注意力机制-P13"><a href="#2-3-2-注意力机制-P13" class="headerlink" title="2.3.2 注意力机制 P13"></a>2.3.2 注意力机制 P13</h3><ul>
<li>学习目标:<ul>
<li>了解什么是注意力计算规则和注意力机制.</li>
<li>掌握注意力计算规则的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是注意力:<ul>
<li>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是注意力计算规则:<ul>
<li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>我们这里使用的注意力的计算规则:<br>$$<br>\mathrm{Attention}(Q,K,V)&#x3D;\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V<br>$$</p>
</li>
<li><p>Q, K, V 的比喻解释:</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">假如我们有一个问题: 给出一段文本，使用一些关键词对它进行描述!<br>为了方便统一正确答案，这道题可能预先已经给大家写出了一些关键词作为提示.其中这些给出的提示就可以看作是key<br>而整个的文本信息就相当于是query，value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的答案信息<br>这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息<br>因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多<br>并且能够开始对我们query也就是这段文本，提取关键信息进行表示.  这就是注意力作用的过程， 通过这个过程，<br>我们最终脑子里的value发生了变化<br>根据提示key生成了query的关键词表示方法，也就是另外一种特征表示方法.<br><br>刚刚我们说到key和value一般情况下默认是相同，与query是不同的，这种是我们一般的注意力输入形式<br><span class="hljs-comment"># 但有一种特殊情况，就是我们query与key和value相同，这种情况我们称为自注意力机制，就如同我们的刚刚的例子</span><br>使用一般注意力机制，是使用不同于给定文本的关键词表示它. 而自注意力机制<br>需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它, 相当于对文本自身的一次特征提取.<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>什么是注意力机制:<ul>
<li>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>注意力机制在网络中实现的图形表示:</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-d7913da8a83d6ca9d67524560b263688_1440w.webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p><code>P14</code> </p>
<ul>
<li>注意力计算规则的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">attention</span>(<span class="hljs-params">query, key, value, mask=<span class="hljs-literal">None</span>, dropout=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;注意力机制的实现, 输入分别是query, key, value, mask: 掩码张量, </span><br><span class="hljs-string">       dropout是nn.Dropout层的实例化对象, 默认为None&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 在函数中, 首先取query的最后一维的大小, 一般情况下就等同于我们的词嵌入维度, 命名为d_k</span><br>    d_k = query.size(-<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 按照注意力公式, 将query与key的转置相乘, 这里面key是将最后两个维度进行转置, 再除以缩放系数根号下d_k, 这种计算方法也称为缩放点积注意力计算.</span><br>    <span class="hljs-comment"># 得到注意力得分张量scores</span><br>    scores = torch.matmul(query, key.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / math.sqrt(d_k)<br><br>    <span class="hljs-comment"># 接着判断是否使用掩码张量</span><br>    <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 使用tensor的masked_fill方法, 将掩码张量和scores张量每个位置一一比较, 如果掩码张量处为0</span><br>        <span class="hljs-comment"># 则对应的scores张量用-1e9这个值来替换, 如下演示</span><br>        scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, -<span class="hljs-number">1e9</span>)<br><br>    <span class="hljs-comment"># 对scores的最后一维进行softmax操作, 使用F.softmax方法, 第一个参数是softmax对象, 第二个是目标维度.</span><br>    <span class="hljs-comment"># 这样获得最终的注意力张量</span><br>    p_attn = F.softmax(scores, dim = -<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 之后判断是否使用dropout进行随机置0</span><br>    <span class="hljs-keyword">if</span> dropout <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 将p_attn传入dropout对象中进行&#x27;丢弃&#x27;处理</span><br>        p_attn = dropout(p_attn)<br><br>    <span class="hljs-comment"># 最后, 根据公式将p_attn与value张量相乘获得最终的query注意力表示, 同时返回注意力张量</span><br>    <span class="hljs-keyword">return</span> torch.matmul(p_attn, value), p_attn<br></code></pre></td></tr></table></figure>

<ul>
<li>tensor.masked_fill演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = Variable(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> <br>Variable containing:<br> <span class="hljs-number">2.0344</span> -<span class="hljs-number">0.5450</span>  <span class="hljs-number">0.3365</span> -<span class="hljs-number">0.1888</span> -<span class="hljs-number">2.1803</span><br> <span class="hljs-number">1.5221</span> -<span class="hljs-number">0.3823</span>  <span class="hljs-number">0.8414</span>  <span class="hljs-number">0.7836</span> -<span class="hljs-number">0.8481</span><br>-<span class="hljs-number">0.0345</span> -<span class="hljs-number">0.8643</span>  <span class="hljs-number">0.6476</span> -<span class="hljs-number">0.2713</span>  <span class="hljs-number">1.5645</span><br> <span class="hljs-number">0.8788</span> -<span class="hljs-number">2.2142</span>  <span class="hljs-number">0.4022</span>  <span class="hljs-number">0.1997</span>  <span class="hljs-number">0.1474</span><br> <span class="hljs-number">2.9109</span>  <span class="hljs-number">0.6006</span> -<span class="hljs-number">0.6745</span> -<span class="hljs-number">1.7262</span>  <span class="hljs-number">0.6977</span><br>[torch.FloatTensor of size 5x5]<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>mask = Variable(torch.zeros(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>mask<br>Variable containing:<br> <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br> <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br> <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br> <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br> <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>[torch.FloatTensor of size 5x5]<br><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span>.masked_fill(mask == <span class="hljs-number">0</span>, -<span class="hljs-number">1e9</span>)<br>Variable containing:<br>-<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span><br>-<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span><br>-<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span><br>-<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span><br>-<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span> -<span class="hljs-number">1.0000e+09</span><br>[torch.FloatTensor of size 5x5]<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 我们令输入的query, key, value都相同, 位置编码的输出</span><br>query = key = value = pe_result<br>Variable containing:<br>( <span class="hljs-number">0</span> ,.,.) = <br>  <span class="hljs-number">46.5196</span>  <span class="hljs-number">16.2057</span> -<span class="hljs-number">41.5581</span>  ...  -<span class="hljs-number">16.0242</span> -<span class="hljs-number">17.8929</span> -<span class="hljs-number">43.0405</span><br> -<span class="hljs-number">32.6040</span>  <span class="hljs-number">16.1096</span> -<span class="hljs-number">29.5228</span>  ...    <span class="hljs-number">4.2721</span>  <span class="hljs-number">20.6034</span>  -<span class="hljs-number">1.2747</span><br> -<span class="hljs-number">18.6235</span>  <span class="hljs-number">14.5076</span>  -<span class="hljs-number">2.0105</span>  ...   <span class="hljs-number">15.6462</span> -<span class="hljs-number">24.6081</span> -<span class="hljs-number">30.3391</span><br>   <span class="hljs-number">0.0000</span> -<span class="hljs-number">66.1486</span> -<span class="hljs-number">11.5123</span>  ...   <span class="hljs-number">20.1519</span>  -<span class="hljs-number">4.6823</span>   <span class="hljs-number">0.4916</span><br><br>( <span class="hljs-number">1</span> ,.,.) = <br> -<span class="hljs-number">24.8681</span>   <span class="hljs-number">7.5495</span>  -<span class="hljs-number">5.0765</span>  ...   -<span class="hljs-number">7.5992</span> -<span class="hljs-number">26.6630</span>  <span class="hljs-number">40.9517</span><br>  <span class="hljs-number">13.1581</span>  -<span class="hljs-number">3.1918</span> -<span class="hljs-number">30.9001</span>  ...   <span class="hljs-number">25.1187</span> -<span class="hljs-number">26.4621</span>   <span class="hljs-number">2.9542</span><br> -<span class="hljs-number">49.7690</span> -<span class="hljs-number">42.5019</span>   <span class="hljs-number">8.0198</span>  ...   -<span class="hljs-number">5.4809</span>  <span class="hljs-number">25.9403</span> -<span class="hljs-number">27.4931</span><br> -<span class="hljs-number">52.2775</span>  <span class="hljs-number">10.4006</span>   <span class="hljs-number">0.0000</span>  ...   -<span class="hljs-number">1.9985</span>   <span class="hljs-number">7.0106</span>  -<span class="hljs-number">0.5189</span><br>[torch.FloatTensor of size 2x4x512]<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">attn, p_attn = attention(query, key, value)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;attn:&quot;</span>, attn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;p_attn:&quot;</span>, p_attn)<br></code></pre></td></tr></table></figure>

<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将得到两个结果</span><br><span class="hljs-comment"># query的注意力表示:</span><br>attn: Variable containing:<br>( <span class="hljs-number">0</span> ,.,.) = <br>   <span class="hljs-number">12.8269</span>    <span class="hljs-number">7.7403</span>   <span class="hljs-number">41.2225</span>  ...     <span class="hljs-number">1.4603</span>   <span class="hljs-number">27.8559</span>  -<span class="hljs-number">12.2600</span><br>   <span class="hljs-number">12.4904</span>    <span class="hljs-number">0.0000</span>   <span class="hljs-number">24.1575</span>  ...     <span class="hljs-number">0.0000</span>    <span class="hljs-number">2.5838</span>   <span class="hljs-number">18.0647</span><br>  -<span class="hljs-number">32.5959</span>   -<span class="hljs-number">4.6252</span>  -<span class="hljs-number">29.1050</span>  ...     <span class="hljs-number">0.0000</span>  -<span class="hljs-number">22.6409</span>  -<span class="hljs-number">11.8341</span><br>    <span class="hljs-number">8.9921</span>  -<span class="hljs-number">33.0114</span>   -<span class="hljs-number">0.7393</span>  ...     <span class="hljs-number">4.7871</span>   -<span class="hljs-number">5.7735</span>    <span class="hljs-number">8.3374</span><br><br>( <span class="hljs-number">1</span> ,.,.) = <br>  -<span class="hljs-number">25.6705</span>   -<span class="hljs-number">4.0860</span>  -<span class="hljs-number">36.8226</span>  ...    <span class="hljs-number">37.2346</span>  -<span class="hljs-number">27.3576</span>    <span class="hljs-number">2.5497</span><br>  -<span class="hljs-number">16.6674</span>   <span class="hljs-number">73.9788</span>  -<span class="hljs-number">33.3296</span>  ...    <span class="hljs-number">28.5028</span>   -<span class="hljs-number">5.5488</span>  -<span class="hljs-number">13.7564</span><br>    <span class="hljs-number">0.0000</span>  -<span class="hljs-number">29.9039</span>   -<span class="hljs-number">3.0405</span>  ...     <span class="hljs-number">0.0000</span>   <span class="hljs-number">14.4408</span>   <span class="hljs-number">14.8579</span><br>   <span class="hljs-number">30.7819</span>    <span class="hljs-number">0.0000</span>   <span class="hljs-number">21.3908</span>  ...   -<span class="hljs-number">29.0746</span>    <span class="hljs-number">0.0000</span>   -<span class="hljs-number">5.8475</span><br>[torch.FloatTensor of size 2x4x512]<br><br><span class="hljs-comment"># 注意力张量:</span><br>p_attn: Variable containing:<br>(<span class="hljs-number">0</span> ,.,.) = <br>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">1</span><br><br>(<span class="hljs-number">1</span> ,.,.) = <br>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span><br>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">1</span><br>[torch.FloatTensor of size 2x4x4]<br></code></pre></td></tr></table></figure>

<ul>
<li>带有mask的输入参数：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">query = key = value = pe_result<br><br><span class="hljs-comment"># 令mask为一个2x4x4的零张量</span><br>mask = Variable(torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">attn, p_attn = attention(query, key, value, mask=mask)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;attn:&quot;</span>, attn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;p_attn:&quot;</span>, p_attn)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>带有mask的输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># query的注意力表示:</span><br>attn: Variable containing:<br>( <span class="hljs-number">0</span> ,.,.) = <br>   <span class="hljs-number">0.4284</span>  -<span class="hljs-number">7.4741</span>   <span class="hljs-number">8.8839</span>  ...    <span class="hljs-number">1.5618</span>   <span class="hljs-number">0.5063</span>   <span class="hljs-number">0.5770</span><br>   <span class="hljs-number">0.4284</span>  -<span class="hljs-number">7.4741</span>   <span class="hljs-number">8.8839</span>  ...    <span class="hljs-number">1.5618</span>   <span class="hljs-number">0.5063</span>   <span class="hljs-number">0.5770</span><br>   <span class="hljs-number">0.4284</span>  -<span class="hljs-number">7.4741</span>   <span class="hljs-number">8.8839</span>  ...    <span class="hljs-number">1.5618</span>   <span class="hljs-number">0.5063</span>   <span class="hljs-number">0.5770</span><br>   <span class="hljs-number">0.4284</span>  -<span class="hljs-number">7.4741</span>   <span class="hljs-number">8.8839</span>  ...    <span class="hljs-number">1.5618</span>   <span class="hljs-number">0.5063</span>   <span class="hljs-number">0.5770</span><br><br>( <span class="hljs-number">1</span> ,.,.) = <br>  -<span class="hljs-number">2.8890</span>   <span class="hljs-number">9.9972</span> -<span class="hljs-number">12.9505</span>  ...    <span class="hljs-number">9.1657</span>  -<span class="hljs-number">4.6164</span>  -<span class="hljs-number">0.5491</span><br>  -<span class="hljs-number">2.8890</span>   <span class="hljs-number">9.9972</span> -<span class="hljs-number">12.9505</span>  ...    <span class="hljs-number">9.1657</span>  -<span class="hljs-number">4.6164</span>  -<span class="hljs-number">0.5491</span><br>  -<span class="hljs-number">2.8890</span>   <span class="hljs-number">9.9972</span> -<span class="hljs-number">12.9505</span>  ...    <span class="hljs-number">9.1657</span>  -<span class="hljs-number">4.6164</span>  -<span class="hljs-number">0.5491</span><br>  -<span class="hljs-number">2.8890</span>   <span class="hljs-number">9.9972</span> -<span class="hljs-number">12.9505</span>  ...    <span class="hljs-number">9.1657</span>  -<span class="hljs-number">4.6164</span>  -<span class="hljs-number">0.5491</span><br>[torch.FloatTensor of size 2x4x512]<br><br><span class="hljs-comment"># 注意力张量:</span><br>p_attn: Variable containing:<br>(<span class="hljs-number">0</span> ,.,.) = <br>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span><br>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span><br>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span><br>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span><br><br>(<span class="hljs-number">1</span> ,.,.) = <br>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span><br>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span><br>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span><br>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span>  <span class="hljs-number">0.2500</span><br>[torch.FloatTensor of size 2x4x4]<br></code></pre></td></tr></table></figure>

<p><code>总结</code>：</p>
<ul>
<li>学习了什么是注意力:<ul>
<li>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是注意力计算规则:<ul>
<li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了<code>Q, K, V</code>的比喻解释:<ul>
<li>Q是一段准备被概括的文本; K是给出的提示; V是大脑中的对提示K的延伸.</li>
<li><code>当Q=K=V时</code>, 称作自注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是注意力机制:<ul>
<li>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了注意力计算规则的函数: attention<ul>
<li>它的输入就是Q，K，V以及mask和dropout, mask用于掩码, dropout用于随机置0.</li>
<li>它的输出有两个, query的注意力表示以及注意力张量.</li>
</ul>
</li>
</ul>
<h3 id="2-3-3-多头注意力机制-P17"><a href="#2-3-3-多头注意力机制-P17" class="headerlink" title="2.3.3 多头注意力机制 P17"></a>2.3.3 多头注意力机制 P17</h3><ul>
<li>学习目标:<ul>
<li>了解多头注意力机制的作用.</li>
<li>掌握多头注意力机制的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是多头注意力机制:<ul>
<li>从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是，我只有使用了一组线性变化层，即<code>三个变换张量对Q，K，V分别进行线性变换</code>，这些变换<code>不会改变原有张量的尺寸</code>，因此每个变换矩阵都是<code>方阵</code>，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>多头注意力机制结构图:</p>
<img src="https://article.biliimg.com/bfs/article/af87236e673d1a082c9941b51678674b155712160.png" srcset="/img/loading.gif" lazyload style="zoom:80%;" /></li>
</ul>
<hr>
<ul>
<li>多头注意力机制的作用:<ul>
<li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>多头注意力机制的代码实现:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 用于深度拷贝的copy工具包</span><br><span class="hljs-keyword">import</span> copy<br><br><span class="hljs-comment"># 首先需要定义克隆函数, 因为在多头注意力机制的实现中, 用到多个结构相同的线性层.</span><br><span class="hljs-comment"># 我们将使用clone函数将他们一同初始化在一个网络层列表对象中. 之后的结构中也会用到该函数.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">clones</span>(<span class="hljs-params">module, N</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;用于生成相同网络层的克隆函数, 它的参数module表示要克隆的目标网络层, N代表需要克隆的数量&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 在函数中, 我们通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层,</span><br>    <span class="hljs-comment"># 然后将其放在nn.ModuleList类型的列表中存放.</span><br>    <span class="hljs-keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N)])<br><br><br><br><br><span class="hljs-comment"># 我们使用一个类来实现多头注意力机制的处理</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadedAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, head, embedding_dim, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;在类的初始化时, 会传入三个参数，head代表头数，embedding_dim代表词嵌入的维度， </span><br><span class="hljs-string">           dropout代表进行dropout操作时置0比率，默认是0.1.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(MultiHeadedAttention, self).__init__()<br><br>        <span class="hljs-comment"># 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，</span><br>        <span class="hljs-comment"># 这是因为我们之后要给每个头分配等量的词特征.也就是embedding_dim/head个.</span><br>        <span class="hljs-keyword">assert</span> embedding_dim % head == <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># 得到每个头获得的分割词向量维度d_k</span><br>        self.d_k = embedding_dim // head<br><br>        <span class="hljs-comment"># 传入头数h</span><br>        self.head = head<br><br>        <span class="hljs-comment"># 然后获得线性层对象，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用clones函数克隆四个，</span><br>        <span class="hljs-comment"># 为什么是四个呢，这是因为在多头注意力中，Q，K，V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个.</span><br>        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), <span class="hljs-number">4</span>)<br><br>        <span class="hljs-comment"># self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None.</span><br>        self.attn = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># 最后就是一个self.dropout对象，它通过nn中的Dropout实例化而来，置0比率为传进来的参数dropout.</span><br>        self.dropout = nn.Dropout(p=dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, query, key, value, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;前向逻辑函数, 它的输入参数有四个，前三个就是注意力机制需要的Q, K, V，</span><br><span class="hljs-string">           最后一个是注意力机制中可能需要的mask掩码张量，默认是None. &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 如果存在掩码张量mask</span><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># 使用unsqueeze拓展维度</span><br>            mask = mask.unsqueeze(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本.</span><br>        batch_size = query.size(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 之后就进入多头处理环节</span><br>        <span class="hljs-comment"># 首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，将输入QKV分别传到线性层中，</span><br>        <span class="hljs-comment"># 做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加了一个维度h，代表头数，</span><br>        <span class="hljs-comment"># 这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，</span><br>        <span class="hljs-comment"># 计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，</span><br>        <span class="hljs-comment"># 为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，</span><br>        <span class="hljs-comment"># 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.</span><br>        query, key, value = \<br>           [model(x).view(batch_size, -<span class="hljs-number">1</span>, self.head, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">for</span> model, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.linears, (query, key, value))]<br><br>        <span class="hljs-comment"># 得到每个头的输入后，接下来就是将他们传入到attention中，</span><br>        <span class="hljs-comment"># 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中.</span><br>        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)<br><br>        <span class="hljs-comment"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，</span><br>        <span class="hljs-comment"># 因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法，</span><br>        <span class="hljs-comment"># 这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，</span><br>        <span class="hljs-comment"># 所以，下一步就是使用view重塑形状，变成和输入形状相同.</span><br>        x = x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(batch_size, -<span class="hljs-number">1</span>, self.head * self.d_k)<br><br>        <span class="hljs-comment"># 最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.</span><br>        <span class="hljs-keyword">return</span> self.linears[-<span class="hljs-number">1</span>](x)<br></code></pre></td></tr></table></figure>

<ul>
<li>tensor.view演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>x.size()<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">4</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>y = x.view(<span class="hljs-number">16</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>y.size()<br>torch.Size([<span class="hljs-number">16</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>z = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">8</span>)  <span class="hljs-comment"># the size -1 is inferred from other dimensions</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>z.size()<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">8</span>])<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>a.size()<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>b = a.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># Swaps 2nd and 3rd dimension</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>b.size()<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>c = a.view(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>)  <span class="hljs-comment"># Does not change tensor layout in memory</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>c.size()<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.equal(b, c)<br><span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure>

<ul>
<li>torch.transpose演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>x<br>tensor([[ <span class="hljs-number">1.0028</span>, -<span class="hljs-number">0.9893</span>,  <span class="hljs-number">0.5809</span>],<br>        [-<span class="hljs-number">0.1669</span>,  <span class="hljs-number">0.7299</span>,  <span class="hljs-number">0.4942</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.transpose(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>tensor([[ <span class="hljs-number">1.0028</span>, -<span class="hljs-number">0.1669</span>],<br>        [-<span class="hljs-number">0.9893</span>,  <span class="hljs-number">0.7299</span>],<br>        [ <span class="hljs-number">0.5809</span>,  <span class="hljs-number">0.4942</span>]])<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 头数head</span><br>head = <span class="hljs-number">8</span><br><br><span class="hljs-comment"># 词嵌入维度embedding_dim</span><br>embedding_dim = <span class="hljs-number">512</span><br><br><span class="hljs-comment"># 置零比率dropout</span><br>dropout = <span class="hljs-number">0.2</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设输入的Q，K，V仍然相等</span><br>query = value = key = pe_result<br><br><span class="hljs-comment"># 输入的掩码张量mask</span><br>mask = Variable(torch.zeros(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">mha = MultiHeadedAttention(head, embedding_dim, dropout)<br>mha_result = mha(query, key, value, mask)<br><span class="hljs-built_in">print</span>(mha_result)<br></code></pre></td></tr></table></figure>

<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[-<span class="hljs-number">0.3075</span>,  <span class="hljs-number">1.5687</span>, -<span class="hljs-number">2.5693</span>,  ..., -<span class="hljs-number">1.1098</span>,  <span class="hljs-number">0.0878</span>, -<span class="hljs-number">3.3609</span>],<br>         [ <span class="hljs-number">3.8065</span>, -<span class="hljs-number">2.4538</span>, -<span class="hljs-number">0.3708</span>,  ..., -<span class="hljs-number">1.5205</span>, -<span class="hljs-number">1.1488</span>, -<span class="hljs-number">1.3984</span>],<br>         [ <span class="hljs-number">2.4190</span>,  <span class="hljs-number">0.5376</span>, -<span class="hljs-number">2.8475</span>,  ...,  <span class="hljs-number">1.4218</span>, -<span class="hljs-number">0.4488</span>, -<span class="hljs-number">0.2984</span>],<br>         [ <span class="hljs-number">2.9356</span>,  <span class="hljs-number">0.3620</span>, -<span class="hljs-number">3.8722</span>,  ..., -<span class="hljs-number">0.7996</span>,  <span class="hljs-number">0.1468</span>,  <span class="hljs-number">1.0345</span>]],<br><br>        [[ <span class="hljs-number">1.1423</span>,  <span class="hljs-number">0.6038</span>,  <span class="hljs-number">0.0954</span>,  ...,  <span class="hljs-number">2.2679</span>, -<span class="hljs-number">5.7749</span>,  <span class="hljs-number">1.4132</span>],<br>         [ <span class="hljs-number">2.4066</span>, -<span class="hljs-number">0.2777</span>,  <span class="hljs-number">2.8102</span>,  ...,  <span class="hljs-number">0.1137</span>, -<span class="hljs-number">3.9517</span>, -<span class="hljs-number">2.9246</span>],<br>         [ <span class="hljs-number">5.8201</span>,  <span class="hljs-number">1.1534</span>, -<span class="hljs-number">1.9191</span>,  ...,  <span class="hljs-number">0.1410</span>, -<span class="hljs-number">7.6110</span>,  <span class="hljs-number">1.0046</span>],<br>         [ <span class="hljs-number">3.1209</span>,  <span class="hljs-number">1.0008</span>, -<span class="hljs-number">0.5317</span>,  ...,  <span class="hljs-number">2.8619</span>, -<span class="hljs-number">6.3204</span>, -<span class="hljs-number">1.3435</span>]]],<br>       grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<p><code>多头注意力机制总结</code>:P 20 – 04:00</p>
<ul>
<li>学习了什么是多头注意力机制:<ul>
<li>每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头.将每个头的获得的输入送到注意力机制中, 就形成了多头注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了多头注意力机制的作用:<ul>
<li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了多头注意力机制的类: MultiHeadedAttention<ul>
<li>因为多头注意力机制中需要使用多个相同的线性层, 首先实现了克隆函数clones.</li>
<li>clones函数的输入是module，N，分别代表克隆的目标层，和克隆个数.</li>
<li>clones函数的输出是装有N个克隆层的Module列表.</li>
<li>接着实现MultiHeadedAttention类, 它的初始化函数输入是h, d_model, dropout分别代表头数，词嵌入维度和置零比率.</li>
<li>它的实例化对象输入是Q, K, V以及掩码张量mask.</li>
<li>它的实例化对象输出是通过多头注意力机制处理的Q的注意力表示.</li>
</ul>
</li>
</ul>
<p><code>P19</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 多头注意力机制的类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadedAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, head, embedding_dim, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-comment"># head:代表几个头的参数</span><br>        <span class="hljs-comment"># embedding_dim:代表词嵌入的维度</span><br>        <span class="hljs-comment"># dropout:进行dropout操作时，置零的比率</span><br>        <span class="hljs-built_in">super</span>(MultiHeadedAttention, self).__init__()<br><br>        <span class="hljs-comment"># 要确认一个事实:多头的数量head需要整除嵌入词的维度embedding_dim</span><br>        <span class="hljs-keyword">assert</span> embedding_dim % head == <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># 获得每个头获得的词向量的维度</span><br>        self.d_k = embedding_dim // head<br><br>        self.head = head<br>        self.embedding_dim = embedding_dim<br><br>        <span class="hljs-comment"># 获得线形层，要获得4个，分别是Q,K,V以及最终的输出线形层</span><br>        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), <span class="hljs-number">4</span>)<br><br>        <span class="hljs-comment"># 初始化注意力张量</span><br>        self.attn = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># 初始化drouput对象</span><br>        self.dropout = nn.Dropout(p=dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, query, key, value, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># query, key, value是注意力机制的三个输入张量，mask代表掩码张量</span><br>        <span class="hljs-comment"># 首先判断是否使用掩码张量</span><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># 使用unsqueeze将掩码张量进行维度扩充，代表多头中的第n个头</span><br>            mask = mask.unsqueeze(<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># 得到batch_size</span><br>        batch_size = query.size(<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 首先使用zip将网络层和输入数据连接在一起，模型的输出利用view和transpose进行维度和形状的改变</span><br>        query, key, value = \<br>            [model(x).view(batch_size, -<span class="hljs-number">1</span>, self.head, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>             <span class="hljs-keyword">for</span> model, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.linears, (query, key, value))]<br>        <br>        <span class="hljs-comment"># 将每个头的输出传入到注意力层</span><br>        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)<br><br>        <span class="hljs-comment"># 得到每个头的计算结果是4维张量，需要进行形状的转换</span><br>        <span class="hljs-comment"># 前面已经将1，2两个维度进行过转置，在这里必须要重新转置回来，</span><br>        <span class="hljs-comment"># 注意：经历了transpose()方法后，必须使用contiguous方法，否则无法使用view(）方法</span><br>        x = x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(batch_size, -<span class="hljs-number">1</span>, self.head * self.d_k)<br>        <br>        <span class="hljs-comment"># 最后将x输入线形层列表中的最后一个线形层中进行处理，得到最终的多头注意力结构输出</span><br>        <span class="hljs-keyword">return</span> self.linears[-<span class="hljs-number">1</span>](x)<br></code></pre></td></tr></table></figure>

<h3 id="2-3-4-前馈全连接层"><a href="#2-3-4-前馈全连接层" class="headerlink" title="2.3.4 前馈全连接层"></a>2.3.4 前馈全连接层</h3><ul>
<li>学习目标:<ul>
<li>了解什么是前馈全连接层及其它的作用.</li>
<li>掌握前馈全连接层的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是前馈全连接层:<ul>
<li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>前馈全连接层的作用:<ul>
<li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>前馈全连接层的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 通过类PositionwiseFeedForward来实现前馈全连接层</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionwiseFeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, d_ff, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-comment"># d_model：代表词嵌入的维度，同时也是两个线性层的输入维度和输出维度</span><br>        <span class="hljs-comment"># d_ff：代表第一个线性层的输出维度，和第二个线性层的输入维度</span><br>        <span class="hljs-comment"># dropout：经过Dropout层处理时，随机置零的比率</span><br>        <span class="hljs-string">&quot;&quot;&quot;初始化函数有三个输入参数分别是d_model, d_ff,和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，</span><br><span class="hljs-string">           因为我们希望输入通过前馈全连接层后输入和输出的维度不变. 第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出维度.</span><br><span class="hljs-string">           最后一个是dropout置0比率.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(PositionwiseFeedForward, self).__init__()<br><br>        <span class="hljs-comment"># 首先按照我们预期使用nn实例化了两个线性层对象，self.w1和self.w2</span><br>        <span class="hljs-comment"># 它们的参数分别是d_model, d_ff和d_ff, d_model</span><br>        self.w1 = nn.Linear(d_model, d_ff)<br>        self.w2 = nn.Linear(d_ff, d_model)<br>        <span class="hljs-comment"># 然后使用nn的Dropout实例化了对象self.dropout</span><br>        self.dropout = nn.Dropout(dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;输入参数为x，代表来自上一层的输出&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 首先经过第一个线性层，然后使用Funtional中relu函数进行激活,</span><br>        <span class="hljs-comment"># 之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果</span><br>        <span class="hljs-keyword">return</span> self.w2(self.dropout(F.relu(self.w1(x))))<br><br>d_model = <span class="hljs-number">512</span><br>d_ff = <span class="hljs-number">64</span><br>dropout = <span class="hljs-number">0.2</span><br><br>x = mha_result<br>ff = PositionwiseFeedForward(d_model, d_ff, dropout)<br>ff_result = ff(x)<br><span class="hljs-built_in">print</span>(ff_result)<br><span class="hljs-built_in">print</span>(ff_result.shape)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>ReLU函数公式: ReLU(x)&#x3D;max(0, x)</li>
</ul>
<hr>
<ul>
<li>ReLU函数图像:</li>
</ul>
<img src="https://article.biliimg.com/bfs/article/7e3ca1eb7eb850ebfa19b2b4039ddd59155712160.png" srcset="/img/loading.gif" lazyload style="zoom:80%;" />



<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">d_model = <span class="hljs-number">512</span><br><br><span class="hljs-comment"># 线性变化的维度</span><br>d_ff = <span class="hljs-number">64</span><br><br>dropout = <span class="hljs-number">0.2</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入参数x可以是多头注意力机制的输出</span><br>x = mha_result<br>tensor([[[-<span class="hljs-number">0.3075</span>,  <span class="hljs-number">1.5687</span>, -<span class="hljs-number">2.5693</span>,  ..., -<span class="hljs-number">1.1098</span>,  <span class="hljs-number">0.0878</span>, -<span class="hljs-number">3.3609</span>],<br>         [ <span class="hljs-number">3.8065</span>, -<span class="hljs-number">2.4538</span>, -<span class="hljs-number">0.3708</span>,  ..., -<span class="hljs-number">1.5205</span>, -<span class="hljs-number">1.1488</span>, -<span class="hljs-number">1.3984</span>],<br>         [ <span class="hljs-number">2.4190</span>,  <span class="hljs-number">0.5376</span>, -<span class="hljs-number">2.8475</span>,  ...,  <span class="hljs-number">1.4218</span>, -<span class="hljs-number">0.4488</span>, -<span class="hljs-number">0.2984</span>],<br>         [ <span class="hljs-number">2.9356</span>,  <span class="hljs-number">0.3620</span>, -<span class="hljs-number">3.8722</span>,  ..., -<span class="hljs-number">0.7996</span>,  <span class="hljs-number">0.1468</span>,  <span class="hljs-number">1.0345</span>]],<br><br>        [[ <span class="hljs-number">1.1423</span>,  <span class="hljs-number">0.6038</span>,  <span class="hljs-number">0.0954</span>,  ...,  <span class="hljs-number">2.2679</span>, -<span class="hljs-number">5.7749</span>,  <span class="hljs-number">1.4132</span>],<br>         [ <span class="hljs-number">2.4066</span>, -<span class="hljs-number">0.2777</span>,  <span class="hljs-number">2.8102</span>,  ...,  <span class="hljs-number">0.1137</span>, -<span class="hljs-number">3.9517</span>, -<span class="hljs-number">2.9246</span>],<br>         [ <span class="hljs-number">5.8201</span>,  <span class="hljs-number">1.1534</span>, -<span class="hljs-number">1.9191</span>,  ...,  <span class="hljs-number">0.1410</span>, -<span class="hljs-number">7.6110</span>,  <span class="hljs-number">1.0046</span>],<br>         [ <span class="hljs-number">3.1209</span>,  <span class="hljs-number">1.0008</span>, -<span class="hljs-number">0.5317</span>,  ...,  <span class="hljs-number">2.8619</span>, -<span class="hljs-number">6.3204</span>, -<span class="hljs-number">1.3435</span>]]],<br>       grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ff = PositionwiseFeedForward(d_model, d_ff, dropout)<br>ff_result = ff(x)<br><span class="hljs-built_in">print</span>(ff_result)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[-<span class="hljs-number">1.9488e+00</span>, -<span class="hljs-number">3.4060e-01</span>, -<span class="hljs-number">1.1216e+00</span>,  ...,  <span class="hljs-number">1.8203e-01</span>,<br>          -<span class="hljs-number">2.6336e+00</span>,  <span class="hljs-number">2.0917e-03</span>],<br>         [-<span class="hljs-number">2.5875e-02</span>,  <span class="hljs-number">1.1523e-01</span>, -<span class="hljs-number">9.5437e-01</span>,  ..., -<span class="hljs-number">2.6257e-01</span>,<br>          -<span class="hljs-number">5.7620e-01</span>, -<span class="hljs-number">1.9225e-01</span>],<br>         [-<span class="hljs-number">8.7508e-01</span>,  <span class="hljs-number">1.0092e+00</span>, -<span class="hljs-number">1.6515e+00</span>,  ...,  <span class="hljs-number">3.4446e-02</span>,<br>          -<span class="hljs-number">1.5933e+00</span>, -<span class="hljs-number">3.1760e-01</span>],<br>         [-<span class="hljs-number">2.7507e-01</span>,  <span class="hljs-number">4.7225e-01</span>, -<span class="hljs-number">2.0318e-01</span>,  ...,  <span class="hljs-number">1.0530e+00</span>,<br>          -<span class="hljs-number">3.7910e-01</span>, -<span class="hljs-number">9.7730e-01</span>]],<br><br>        [[-<span class="hljs-number">2.2575e+00</span>, -<span class="hljs-number">2.0904e+00</span>,  <span class="hljs-number">2.9427e+00</span>,  ...,  <span class="hljs-number">9.6574e-01</span>,<br>          -<span class="hljs-number">1.9754e+00</span>,  <span class="hljs-number">1.2797e+00</span>],<br>         [-<span class="hljs-number">1.5114e+00</span>, -<span class="hljs-number">4.7963e-01</span>,  <span class="hljs-number">1.2881e+00</span>,  ..., -<span class="hljs-number">2.4882e-02</span>,<br>          -<span class="hljs-number">1.5896e+00</span>, -<span class="hljs-number">1.0350e+00</span>],<br>         [ <span class="hljs-number">1.7416e-01</span>, -<span class="hljs-number">4.0688e-01</span>,  <span class="hljs-number">1.9289e+00</span>,  ..., -<span class="hljs-number">4.9754e-01</span>,<br>          -<span class="hljs-number">1.6320e+00</span>, -<span class="hljs-number">1.5217e+00</span>],<br>         [-<span class="hljs-number">1.0874e-01</span>, -<span class="hljs-number">3.3842e-01</span>,  <span class="hljs-number">2.9379e-01</span>,  ..., -<span class="hljs-number">5.1276e-01</span>,<br>          -<span class="hljs-number">1.6150e+00</span>, -<span class="hljs-number">1.1295e+00</span>]]], grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<p><code>前馈全连接层总结</code>:</p>
<ul>
<li>学习了什么是前馈全连接层:<ul>
<li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了前馈全连接层的作用:<ul>
<li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了前馈全连接层的类: PositionwiseFeedForward<ul>
<li>它的实例化参数为d_model, d_ff, dropout, 分别代表词嵌入维度, 线性变换维度, 和置零比率.</li>
<li>它的输入参数x, 表示上层的输出.</li>
<li>它的输出是经过2层线性网络变换的特征表示.</li>
</ul>
</li>
</ul>
<h3 id="2-3-5-规范化层"><a href="#2-3-5-规范化层" class="headerlink" title="2.3.5 规范化层"></a>2.3.5 规范化层</h3><ul>
<li>学习目标:<ul>
<li>了解规范化层的作用.</li>
<li>掌握规范化层的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>规范化层的作用:<ul>
<li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>规范化层的代码实现:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"> 通过LayerNorm实现规范化层的类<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, features, eps=<span class="hljs-number">1e-6</span></span>):<br>        <span class="hljs-comment"># features: 词嵌入的维度</span><br>        <span class="hljs-comment"># eps： 一个足够小的正数，用来在规范化计算公式的分母中，放置除零操作</span><br>        <span class="hljs-string">&quot;&quot;&quot;初始化函数有两个参数, 一个是features, 表示词嵌入的维度,</span><br><span class="hljs-string">           另一个是eps它是一个足够小的数, 在规范化公式的分母中出现,</span><br><span class="hljs-string">           防止分母为0.默认是1e-6.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(LayerNorm, self).__init__()<br><br>        <span class="hljs-comment"># 根据features的形状初始化两个参数张量a2，和b2，第一个初始化为1张量，</span><br>        <span class="hljs-comment"># 也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，这两个张量就是规范化层的参数，</span><br>        <span class="hljs-comment"># 因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，</span><br>        <span class="hljs-comment"># 使其即能满足规范化要求，又能不改变针对目标的表征.最后使用nn.parameter封装，代表他们是模型的参数。</span><br>        self.a2 = nn.Parameter(torch.ones(features))<br>        self.b2 = nn.Parameter(torch.zeros(features))<br><br>        <span class="hljs-comment"># 把eps传到类中</span><br>        self.eps = eps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x：上一层网络的输出</span><br>        <span class="hljs-comment"># 首先对x进行最后一个维度上的求均值操作，同时保持输出维度和输入维度一直</span><br>        <span class="hljs-string">&quot;&quot;&quot;输入参数x代表来自上一层的输出&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致.</span><br>        <span class="hljs-comment"># 接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果，</span><br>        <span class="hljs-comment"># 最后对结果乘以我们的缩放参数，即a2，*号代表同型点乘，即对应位置进行乘法操作，加上位移参数b2.返回即可.</span><br>        mean = x.mean(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)	<span class="hljs-comment"># -1:在最后一个维度上</span><br>        <span class="hljs-comment"># 按照规范化公式进行计算并返回</span><br>        std = x.std(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> self.a2 * (x - mean) / (std + self.eps) + self.b2<br><br><br>features = d_model = <span class="hljs-number">512</span><br>eps = <span class="hljs-number">1e-6</span><br><br>x = ff_result<br>ln = LayerNorm(features, eps)<br>ln_result = ln(x)<br><span class="hljs-built_in">print</span>(ln_result)<br><span class="hljs-built_in">print</span>(ln_result.shape)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">features = d_model = <span class="hljs-number">512</span><br>eps = <span class="hljs-number">1e-6</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入x来自前馈全连接层的输出</span><br>x = ff_result<br>tensor([[[-<span class="hljs-number">1.9488e+00</span>, -<span class="hljs-number">3.4060e-01</span>, -<span class="hljs-number">1.1216e+00</span>,  ...,  <span class="hljs-number">1.8203e-01</span>,<br>          -<span class="hljs-number">2.6336e+00</span>,  <span class="hljs-number">2.0917e-03</span>],<br>         [-<span class="hljs-number">2.5875e-02</span>,  <span class="hljs-number">1.1523e-01</span>, -<span class="hljs-number">9.5437e-01</span>,  ..., -<span class="hljs-number">2.6257e-01</span>,<br>          -<span class="hljs-number">5.7620e-01</span>, -<span class="hljs-number">1.9225e-01</span>],<br>         [-<span class="hljs-number">8.7508e-01</span>,  <span class="hljs-number">1.0092e+00</span>, -<span class="hljs-number">1.6515e+00</span>,  ...,  <span class="hljs-number">3.4446e-02</span>,<br>          -<span class="hljs-number">1.5933e+00</span>, -<span class="hljs-number">3.1760e-01</span>],<br>         [-<span class="hljs-number">2.7507e-01</span>,  <span class="hljs-number">4.7225e-01</span>, -<span class="hljs-number">2.0318e-01</span>,  ...,  <span class="hljs-number">1.0530e+00</span>,<br>          -<span class="hljs-number">3.7910e-01</span>, -<span class="hljs-number">9.7730e-01</span>]],<br><br>        [[-<span class="hljs-number">2.2575e+00</span>, -<span class="hljs-number">2.0904e+00</span>,  <span class="hljs-number">2.9427e+00</span>,  ...,  <span class="hljs-number">9.6574e-01</span>,<br>          -<span class="hljs-number">1.9754e+00</span>,  <span class="hljs-number">1.2797e+00</span>],<br>         [-<span class="hljs-number">1.5114e+00</span>, -<span class="hljs-number">4.7963e-01</span>,  <span class="hljs-number">1.2881e+00</span>,  ..., -<span class="hljs-number">2.4882e-02</span>,<br>          -<span class="hljs-number">1.5896e+00</span>, -<span class="hljs-number">1.0350e+00</span>],<br>         [ <span class="hljs-number">1.7416e-01</span>, -<span class="hljs-number">4.0688e-01</span>,  <span class="hljs-number">1.9289e+00</span>,  ..., -<span class="hljs-number">4.9754e-01</span>,<br>          -<span class="hljs-number">1.6320e+00</span>, -<span class="hljs-number">1.5217e+00</span>],<br>         [-<span class="hljs-number">1.0874e-01</span>, -<span class="hljs-number">3.3842e-01</span>,  <span class="hljs-number">2.9379e-01</span>,  ..., -<span class="hljs-number">5.1276e-01</span>,<br>          -<span class="hljs-number">1.6150e+00</span>, -<span class="hljs-number">1.1295e+00</span>]]], grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ln = LayerNorm(feature, eps)<br>ln_result = ln(x)<br><span class="hljs-built_in">print</span>(ln_result)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[ <span class="hljs-number">2.2697</span>,  <span class="hljs-number">1.3911</span>, -<span class="hljs-number">0.4417</span>,  ...,  <span class="hljs-number">0.9937</span>,  <span class="hljs-number">0.6589</span>, -<span class="hljs-number">1.1902</span>],<br>         [ <span class="hljs-number">1.5876</span>,  <span class="hljs-number">0.5182</span>,  <span class="hljs-number">0.6220</span>,  ...,  <span class="hljs-number">0.9836</span>,  <span class="hljs-number">0.0338</span>, -<span class="hljs-number">1.3393</span>],<br>         [ <span class="hljs-number">1.8261</span>,  <span class="hljs-number">2.0161</span>,  <span class="hljs-number">0.2272</span>,  ...,  <span class="hljs-number">0.3004</span>,  <span class="hljs-number">0.5660</span>, -<span class="hljs-number">0.9044</span>],<br>         [ <span class="hljs-number">1.5429</span>,  <span class="hljs-number">1.3221</span>, -<span class="hljs-number">0.2933</span>,  ...,  <span class="hljs-number">0.0406</span>,  <span class="hljs-number">1.0603</span>,  <span class="hljs-number">1.4666</span>]],<br><br>        [[ <span class="hljs-number">0.2378</span>,  <span class="hljs-number">0.9952</span>,  <span class="hljs-number">1.2621</span>,  ..., -<span class="hljs-number">0.4334</span>, -<span class="hljs-number">1.1644</span>,  <span class="hljs-number">1.2082</span>],<br>         [-<span class="hljs-number">1.0209</span>,  <span class="hljs-number">0.6435</span>,  <span class="hljs-number">0.4235</span>,  ..., -<span class="hljs-number">0.3448</span>, -<span class="hljs-number">1.0560</span>,  <span class="hljs-number">1.2347</span>],<br>         [-<span class="hljs-number">0.8158</span>,  <span class="hljs-number">0.7118</span>,  <span class="hljs-number">0.4110</span>,  ...,  <span class="hljs-number">0.0990</span>, -<span class="hljs-number">1.4833</span>,  <span class="hljs-number">1.9434</span>],<br>         [ <span class="hljs-number">0.9857</span>,  <span class="hljs-number">2.3924</span>,  <span class="hljs-number">0.3819</span>,  ...,  <span class="hljs-number">0.0157</span>, -<span class="hljs-number">1.6300</span>,  <span class="hljs-number">1.2251</span>]]],<br>       grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<p><code>规范化层总结</code>:</p>
<ul>
<li>学习了规范化层的作用:<ul>
<li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了规范化层的类: LayerNorm<ul>
<li>它的实例化参数有两个, features和eps，分别表示词嵌入特征大小，和一个足够小的数.</li>
<li>它的输入参数x代表来自上一层的输出.</li>
<li>它的输出就是经过规范化的特征表示.</li>
</ul>
</li>
</ul>
<h3 id="2-3-6-子层连接结构"><a href="#2-3-6-子层连接结构" class="headerlink" title="2.3.6 子层连接结构"></a>2.3.6 子层连接结构</h3><ul>
<li>学习目标:<ul>
<li>了解什么是子层连接结构.</li>
<li>掌握子层连接结构的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是子层连接结构:<ul>
<li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构），在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>子层连接结构图:</p>
<p><img src="https://article.biliimg.com/bfs/article/122092d91f732d6ea7ef61d100d3f1fc155712160.png" srcset="/img/loading.gif" lazyload></p>
</li>
</ul>
<hr>
<p><img src="https://article.biliimg.com/bfs/article/442c5ecb840fdeae191a0184e836f395155712160.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>子层连接结构的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用SublayerConnection来实现子层连接结构的类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SublayerConnection</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, size, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;它输入参数有两个, size以及dropout， size一般是都是词嵌入维度的大小， </span><br><span class="hljs-string">           dropout本身是对模型结构中的节点数进行随机抑制的比率， </span><br><span class="hljs-string">           又因为节点被抑制等效就是该节点的输出都是0，因此也可以把dropout看作是对输出矩阵的随机置0的比率.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(SublayerConnection, self).__init__()<br>        <span class="hljs-comment"># 实例化了规范化对象self.norm</span><br>        self.norm = LayerNorm(size)<br>        <span class="hljs-comment"># 又使用nn中预定义的droupout实例化一个self.dropout对象.</span><br>        self.dropout = nn.Dropout(p=dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, sublayer</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;前向逻辑函数中, 接收上一个层或者子层的输入作为第一个参数，</span><br><span class="hljs-string">           将该子层连接中的子层函数作为第二个参数&quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 我们首先对输出进行规范化，然后将结果传给子层处理，之后再对子层进行dropout操作，</span><br>        <span class="hljs-comment"># 随机停止一些网络中神经元的作用，来防止过拟合. 最后还有一个add操作， </span><br>        <span class="hljs-comment"># 因为存在跳跃连接，所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出.</span><br>        <span class="hljs-keyword">return</span> x + self.dropout(sublayer(self.norm(x)))<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>实例化参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">size = <span class="hljs-number">512</span><br>dropout = <span class="hljs-number">0.2</span><br>head = <span class="hljs-number">8</span><br>d_model = <span class="hljs-number">512</span><br><span class="hljs-comment">###################################################</span><br>size = <span class="hljs-number">512</span><br>dropout = <span class="hljs-number">0.2</span><br>head = <span class="hljs-number">8</span><br>d_model = <span class="hljs-number">512</span><br>x = pe_result<br>mask = Variable(torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br>self_attn = MultiHeadedAttention(head, d_model)<br><br>sublayer = <span class="hljs-keyword">lambda</span> x: self_attn(x, x, x, mask)<br><br>sc = SublayerConnection(size, dropout)<br>sc_result = sc(x, sublayer)<br><span class="hljs-built_in">print</span>(sc_result)<br><span class="hljs-built_in">print</span>(sc_result.shape)<br></code></pre></td></tr></table></figure>

<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 令x为位置编码器的输出</span><br>x = pe_result<br>mask = Variable(torch.zeros(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br><br><span class="hljs-comment"># 假设子层中装的是多头注意力层, 实例化这个类</span><br>self_attn =  MultiHeadedAttention(head, d_model)<br><br><span class="hljs-comment"># 使用lambda获得一个函数类型的子层</span><br>sublayer = <span class="hljs-keyword">lambda</span> x: self_attn(x, x, x, mask)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">sc = SublayerConnection(size, dropout)<br>sc_result = sc(x, sublayer)<br><span class="hljs-built_in">print</span>(sc_result)<br><span class="hljs-built_in">print</span>(sc_result.shape)<br></code></pre></td></tr></table></figure>

<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[ <span class="hljs-number">14.8830</span>,  <span class="hljs-number">22.4106</span>, -<span class="hljs-number">31.4739</span>,  ...,  <span class="hljs-number">21.0882</span>, -<span class="hljs-number">10.0338</span>,  -<span class="hljs-number">0.2588</span>],<br>         [-<span class="hljs-number">25.1435</span>,   <span class="hljs-number">2.9246</span>, -<span class="hljs-number">16.1235</span>,  ...,  <span class="hljs-number">10.5069</span>,  -<span class="hljs-number">7.1007</span>,  -<span class="hljs-number">3.7396</span>],<br>         [  <span class="hljs-number">0.1374</span>,  <span class="hljs-number">32.6438</span>,  <span class="hljs-number">12.3680</span>,  ..., -<span class="hljs-number">12.0251</span>, -<span class="hljs-number">40.5829</span>,   <span class="hljs-number">2.2297</span>],<br>         [-<span class="hljs-number">13.3123</span>,  <span class="hljs-number">55.4689</span>,   <span class="hljs-number">9.5420</span>,  ..., -<span class="hljs-number">12.6622</span>,  <span class="hljs-number">23.4496</span>,  <span class="hljs-number">21.1531</span>]],<br><br>        [[ <span class="hljs-number">13.3533</span>,  <span class="hljs-number">17.5674</span>, -<span class="hljs-number">13.3354</span>,  ...,  <span class="hljs-number">29.1366</span>,  -<span class="hljs-number">6.4898</span>,  <span class="hljs-number">35.8614</span>],<br>         [-<span class="hljs-number">35.2286</span>,  <span class="hljs-number">18.7378</span>, -<span class="hljs-number">31.4337</span>,  ...,  <span class="hljs-number">11.1726</span>,  <span class="hljs-number">20.6372</span>,  <span class="hljs-number">29.8689</span>],<br>         [-<span class="hljs-number">30.7627</span>,   <span class="hljs-number">0.0000</span>, -<span class="hljs-number">57.0587</span>,  ...,  <span class="hljs-number">15.0724</span>, -<span class="hljs-number">10.7196</span>, -<span class="hljs-number">18.6290</span>],<br>         [ -<span class="hljs-number">2.7757</span>, -<span class="hljs-number">19.6408</span>,   <span class="hljs-number">0.0000</span>,  ...,  <span class="hljs-number">12.7660</span>,  <span class="hljs-number">21.6843</span>, -<span class="hljs-number">35.4784</span>]]],<br>       grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<p><code>子层连接结构总结</code>:</p>
<ul>
<li>什么是子层连接结构:<ul>
<li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构）, 在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了子层连接结构的类: SublayerConnection<ul>
<li>类的初始化函数输入参数是size, dropout, 分别代表<code>词嵌入大小</code>和<code>置零比率</code>.</li>
<li>它的实例化对象输入参数是x, sublayer, 分别代表上一层输出以及子层的函数表示.</li>
<li>它的输出就是通过子层连接结构处理的输出.</li>
</ul>
</li>
</ul>
<h3 id="2-3-7-编码器层"><a href="#2-3-7-编码器层" class="headerlink" title="2.3.7 编码器层"></a>2.3.7 编码器层</h3><ul>
<li><p>学习目标:</p>
<ul>
<li><p>了解编码器层的作用.</p>
</li>
<li><p>掌握编码器层的实现过程.</p>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器层的作用:<ul>
<li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器层的构成图:</li>
</ul>
<img src="https://article.biliimg.com/bfs/article/f46db8ec8cc3f5cc82fe1a014104a212155712160.png" srcset="/img/loading.gif" lazyload style="zoom: 67%;" />

<ul>
<li>编码器层的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"> 使用EncoderLayer类实现编码器层 构建编码器层的类<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, size, self_attn, feed_forward, dropout</span>):<br>        <span class="hljs-comment"># size：代表词嵌入的维度</span><br>        <span class="hljs-comment"># self_at tn：代表传入的多头自注意力子层的实例化对象</span><br>        <span class="hljs-comment"># feed_forward：代表前馈全连接层实例化对象</span><br>        <span class="hljs-comment"># dropout：进行dropout操作时的置零比率</span><br><br>        <span class="hljs-string">&quot;&quot;&quot;它的初始化函数参数有四个，分别是size，其实就是我们词嵌入维度的大小，它也将作为我们编码器层的大小,</span><br><span class="hljs-string">           第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制,</span><br><span class="hljs-string">           第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象, 最后一个是置0比率dropout.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(EncoderLayer, self).__init__()<br><br>        <span class="hljs-comment"># 首先将self_attn和feed_forward传入其中.</span><br>        <span class="hljs-comment"># 将两个实例化对象和参数传入类中</span><br><br>        self.self_attn = self_attn<br>        self.feed_forward = feed_forward<br><br>        <span class="hljs-comment"># 如图所示, 编码器层中有两个子层连接结构, 所以使用clones函数进行克隆操作</span><br>        self.sublayer = clones(SublayerConnection(size, dropout), <span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 把size传入其中</span><br>        self.size = size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask</span>):<br>        <span class="hljs-comment"># X：代表上一层的传入张量</span><br>        <span class="hljs-comment"># mask：代表掩码张量</span><br>        <span class="hljs-comment"># 首先让经过第一个子层连接结构，内部包含多头自注意力机制子层</span><br>        <span class="hljs-comment"># 再让张量经过第二个子层连接结构，其中包含前馈全连接网络</span><br><br>        <span class="hljs-comment"># forward函数中有两个输入参数，x和mask，分别代表上一层的输出，和掩码张量mask.</span><br>        <span class="hljs-comment"># 里面就是按照结构图左侧的流程. 首先通过第一个子层连接结构，其中包含多头自注意力子层，</span><br>        <span class="hljs-comment"># 然后通过第二个子层连接结构，其中包含前馈全连接子层. 最后返回结果.</span><br>        x = self.sublayer[<span class="hljs-number">0</span>](x, <span class="hljs-keyword">lambda</span> x: self.self_attn(x, x, x, mask))<br>        <span class="hljs-keyword">return</span> self.sublayer[<span class="hljs-number">1</span>](x, self.feed_forward)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><br>size = <span class="hljs-number">512</span><br>head = <span class="hljs-number">8</span><br>d_model = <span class="hljs-number">512</span><br>d_ff = <span class="hljs-number">64</span><br>x = pe_result<br>dropout = <span class="hljs-number">0.2</span><br>self_attn = MultiHeadedAttention(head, d_model)<br>ff = PositionwiseFeedForward(d_model, d_ff, dropout)<br>mask = Variable(torch.zeros(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>

<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">el = EncoderLayer(size, self_attn, ff, dropout)<br>el_result = el(x, mask)<br><span class="hljs-built_in">print</span>(el_result)<br><span class="hljs-built_in">print</span>(el_result.shape)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[ <span class="hljs-number">33.6988</span>, -<span class="hljs-number">30.7224</span>,  <span class="hljs-number">20.9575</span>,  ...,   <span class="hljs-number">5.2968</span>, -<span class="hljs-number">48.5658</span>,  <span class="hljs-number">20.0734</span>],<br>         [-<span class="hljs-number">18.1999</span>,  <span class="hljs-number">34.2358</span>,  <span class="hljs-number">40.3094</span>,  ...,  <span class="hljs-number">10.1102</span>,  <span class="hljs-number">58.3381</span>,  <span class="hljs-number">58.4962</span>],<br>         [ <span class="hljs-number">32.1243</span>,  <span class="hljs-number">16.7921</span>,  -<span class="hljs-number">6.8024</span>,  ...,  <span class="hljs-number">23.0022</span>, -<span class="hljs-number">18.1463</span>, -<span class="hljs-number">17.1263</span>],<br>         [ -<span class="hljs-number">9.3475</span>,  -<span class="hljs-number">3.3605</span>, -<span class="hljs-number">55.3494</span>,  ...,  <span class="hljs-number">43.6333</span>,  -<span class="hljs-number">0.1900</span>,   <span class="hljs-number">0.1625</span>]],<br><br>        [[ <span class="hljs-number">32.8937</span>, -<span class="hljs-number">46.2808</span>,   <span class="hljs-number">8.5047</span>,  ...,  <span class="hljs-number">29.1837</span>,  <span class="hljs-number">22.5962</span>, -<span class="hljs-number">14.4349</span>],<br>         [ <span class="hljs-number">21.3379</span>,  <span class="hljs-number">20.0657</span>, -<span class="hljs-number">31.7256</span>,  ..., -<span class="hljs-number">13.4079</span>, -<span class="hljs-number">44.0706</span>,  -<span class="hljs-number">9.9504</span>],<br>         [ <span class="hljs-number">19.7478</span>,  -<span class="hljs-number">1.0848</span>,  <span class="hljs-number">11.8884</span>,  ...,  -<span class="hljs-number">9.5794</span>,   <span class="hljs-number">0.0675</span>,  -<span class="hljs-number">4.7123</span>],<br>         [ -<span class="hljs-number">6.8023</span>, -<span class="hljs-number">16.1176</span>,  <span class="hljs-number">20.9476</span>,  ...,  -<span class="hljs-number">6.5469</span>,  <span class="hljs-number">34.8391</span>, -<span class="hljs-number">14.9798</span>]]],<br>       grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<p><code>编码器层总结</code>：</p>
<ul>
<li>学习了编码器层的作用:<ul>
<li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了编码器层的类: EncoderLayer<ul>
<li>类的初始化函数共有4个, 别是size，其实就是我们词嵌入维度的大小. 第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制. 第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象. 最后一个是置0比率dropout.<ul>
<li>实例化对象的输入参数有2个，x代表来自上一层的输出, mask代表掩码张量.</li>
<li>它的输出代表经过整个编码层的特征表示.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-3-8-编码器"><a href="#2-3-8-编码器" class="headerlink" title="2.3.8 编码器"></a>2.3.8 编码器</h3><ul>
<li>学习目标:<ul>
<li>了解编码器的作用.</li>
<li>掌握编码器的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器的作用:<ul>
<li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器的结构图:</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-306c89830ea9efe5953ae4d8b76492a4_1440w.webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<ul>
<li>编码器的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用Encoder类来实现编码器</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, layer, N</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;初始化函数的两个参数分别代表编码器层和编码器层的个数&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(Encoder, self).__init__()<br>        <span class="hljs-comment"># 首先使用clones函数克隆N个编码器层放在self.layers中</span><br>        self.layers = clones(layer, N)<br>        <span class="hljs-comment"># 再初始化一个规范化层, 它将用在编码器的最后面.</span><br>        self.norm = LayerNorm(layer.size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;forward函数的输入和编码器层相同, x代表上一层的输出, mask代表掩码张量&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 首先就是对我们克隆的编码器层进行循环，每次都会得到一个新的x，</span><br>        <span class="hljs-comment"># 这个循环的过程，就相当于输出的x经过了N个编码器层的处理. </span><br>        <span class="hljs-comment"># 最后再通过规范化层的对象self.norm进行处理，最后返回结果. </span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>            x = layer(x, mask)<br>        <span class="hljs-keyword">return</span> self.norm(x)<br></code></pre></td></tr></table></figure>

<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第一个实例化参数layer, 它是一个编码器层的实例化对象, 因此需要传入编码器层的参数</span><br><span class="hljs-comment"># 又因为编码器层中的子层是不共享的, 因此需要使用深度拷贝各个对象.</span><br>size = <span class="hljs-number">512</span><br>head = <span class="hljs-number">8</span><br>d_model = <span class="hljs-number">512</span><br>d_ff = <span class="hljs-number">64</span><br>c = copy.deepcopy<br>attn = MultiHeadedAttention(head, d_model)<br>ff = PositionwiseFeedForward(d_model, d_ff, dropout)<br>dropout = <span class="hljs-number">0.2</span><br>layer = EncoderLayer(size, c(attn), c(ff), dropout)<br><br><span class="hljs-comment"># 编码器中编码器层的个数N</span><br>N = <span class="hljs-number">8</span><br>mask = Variable(torch.zeros(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">en = Encoder(layer, N)<br>en_result = en(x, mask)<br><span class="hljs-built_in">print</span>(en_result)<br><span class="hljs-built_in">print</span>(en_result.shape)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[-<span class="hljs-number">0.2081</span>, -<span class="hljs-number">0.3586</span>, -<span class="hljs-number">0.2353</span>,  ...,  <span class="hljs-number">2.5646</span>, -<span class="hljs-number">0.2851</span>,  <span class="hljs-number">0.0238</span>],<br>         [ <span class="hljs-number">0.7957</span>, -<span class="hljs-number">0.5481</span>,  <span class="hljs-number">1.2443</span>,  ...,  <span class="hljs-number">0.7927</span>,  <span class="hljs-number">0.6404</span>, -<span class="hljs-number">0.0484</span>],<br>         [-<span class="hljs-number">0.1212</span>,  <span class="hljs-number">0.4320</span>, -<span class="hljs-number">0.5644</span>,  ...,  <span class="hljs-number">1.3287</span>, -<span class="hljs-number">0.0935</span>, -<span class="hljs-number">0.6861</span>],<br>         [-<span class="hljs-number">0.3937</span>, -<span class="hljs-number">0.6150</span>,  <span class="hljs-number">2.2394</span>,  ..., -<span class="hljs-number">1.5354</span>,  <span class="hljs-number">0.7981</span>,  <span class="hljs-number">1.7907</span>]],<br><br>        [[-<span class="hljs-number">2.3005</span>,  <span class="hljs-number">0.3757</span>,  <span class="hljs-number">1.0360</span>,  ...,  <span class="hljs-number">1.4019</span>,  <span class="hljs-number">0.6493</span>, -<span class="hljs-number">0.1467</span>],<br>         [ <span class="hljs-number">0.5653</span>,  <span class="hljs-number">0.1569</span>,  <span class="hljs-number">0.4075</span>,  ..., -<span class="hljs-number">0.3205</span>,  <span class="hljs-number">1.4774</span>, -<span class="hljs-number">0.5856</span>],<br>         [-<span class="hljs-number">1.0555</span>,  <span class="hljs-number">0.0061</span>, -<span class="hljs-number">1.8165</span>,  ..., -<span class="hljs-number">0.4339</span>, -<span class="hljs-number">1.8780</span>,  <span class="hljs-number">0.2467</span>],<br>         [-<span class="hljs-number">2.1617</span>, -<span class="hljs-number">1.5532</span>, -<span class="hljs-number">1.4330</span>,  ..., -<span class="hljs-number">0.9433</span>, -<span class="hljs-number">0.5304</span>, -<span class="hljs-number">1.7022</span>]]],<br>       grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<p><strong>编码器总结：</strong></p>
<ul>
<li>学习了编码器的作用:<ul>
<li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了编码器的类: Encoder<ul>
<li>类的初始化函数参数有两个，分别是layer和N，代表编码器层和编码器层的个数.</li>
<li>forward函数的输入参数也有两个, 和编码器层的forward相同, x代表上一层的输出, mask代码掩码张量.</li>
<li>编码器类的输出就是Transformer中编码器的特征提取表示, 它将成为解码器的输入的一部分.</li>
</ul>
</li>
</ul>
<h2 id="2-4-解码器部分实现"><a href="#2-4-解码器部分实现" class="headerlink" title="2.4 解码器部分实现"></a>2.4 解码器部分实现</h2><ul>
<li><p>学习目标</p>
<ul>
<li><p>了解解码器中各个组成部分的作用.</p>
</li>
<li><p>掌握解码器中各个组成部分的实现过程.</p>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>解码器部分:</p>
<ul>
<li>由N个解码器层堆叠而成</li>
<li>每个解码器层由三个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和<code>规范化层以及一个残差连接</code></li>
<li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li>
<li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-4e17a521400c92b41a5927e5c40aeef0_1440w.webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>说明:</p>
<ul>
<li>解码器层中的各个部分，如，多头注意力机制，规范化层，前馈全连接网络，子层连接结构都与编码器中的实现相同. 因此这里可以直接拿来构建解码器层.</li>
</ul>
<hr>
<h3 id="2-4-1-解码器层"><a href="#2-4-1-解码器层" class="headerlink" title="2.4.1 解码器层"></a>2.4.1 解码器层</h3><ul>
<li>学习目标:<ul>
<li>了解解码器的作用.</li>
<li>掌握解码器的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>解码器的作用:<ul>
<li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>解码器层的代码实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用DecoderLayer的类实现解码器层</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, size, self_attn, src_attn, feed_forward, dropout</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，</span><br><span class="hljs-string">            第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V， </span><br><span class="hljs-string">            第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(DecoderLayer, self).__init__()<br>        <span class="hljs-comment"># 在初始化函数中， 主要就是将这些输入传到类中</span><br>        self.size = size<br>        self.self_attn = self_attn<br>        self.src_attn = src_attn<br>        self.feed_forward = feed_forward<br>        <span class="hljs-comment"># 按照结构图使用clones函数克隆三个子层连接对象.</span><br>        self.sublayer = clones(SublayerConnection(size, dropout), <span class="hljs-number">3</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, memory, source_mask, target_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;forward函数中的参数有4个，分别是来自上一层的输入x，</span><br><span class="hljs-string">           来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 将memory表示成m方便之后使用</span><br>        m = memory<br><br>        <span class="hljs-comment"># 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，</span><br>        <span class="hljs-comment"># 最后一个参数是目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据，</span><br>        <span class="hljs-comment"># 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，</span><br>        <span class="hljs-comment"># 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，</span><br>        <span class="hljs-comment"># 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用.</span><br>        x = self.sublayer[<span class="hljs-number">0</span>](x, <span class="hljs-keyword">lambda</span> x: self.self_attn(x, x, x, target_mask))<br><br>        <span class="hljs-comment"># 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k，v是编码层输出memory， </span><br>        <span class="hljs-comment"># 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，而是遮蔽掉对结果没有意义的字符而产生的注意力值，</span><br>        <span class="hljs-comment"># 以此提升模型效果和训练速度. 这样就完成了第二个子层的处理.</span><br>        x = self.sublayer[<span class="hljs-number">1</span>](x, <span class="hljs-keyword">lambda</span> x: self.src_attn(x, m, m, source_mask))<br><br>        <span class="hljs-comment"># 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果.这就是我们的解码器层结构.</span><br>        <span class="hljs-keyword">return</span> self.sublayer[<span class="hljs-number">2</span>](x, self.feed_forward)<br></code></pre></td></tr></table></figure>

<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 类的实例化参数与解码器层类似, 相比多出了src_attn, 但是和self_attn是同一个类.</span><br>head = <span class="hljs-number">8</span><br>size = <span class="hljs-number">512</span><br>d_model = <span class="hljs-number">512</span><br>d_ff = <span class="hljs-number">64</span><br>dropout = <span class="hljs-number">0.2</span><br>self_attn = src_attn = MultiHeadedAttention(head, d_model, dropout)<br><br><span class="hljs-comment"># 前馈全连接层也和之前相同 </span><br>ff = PositionwiseFeedForward(d_model, d_ff, dropout)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># x是来自目标数据的词嵌入表示, 但形式和源数据的词嵌入表示相同, 这里使用per充当.</span><br>x = pe_result<br><br><span class="hljs-comment"># memory是来自编码器的输出</span><br>memory = en_result<br><br><span class="hljs-comment"># 实际中source_mask和target_mask并不相同, 这里为了方便计算使他们都为mask</span><br>mask = Variable(torch.zeros(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br>source_mask = target_mask = mask<br></code></pre></td></tr></table></figure>

<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)<br>dl_result = dl(x, memory, source_mask, target_mask)<br><span class="hljs-built_in">print</span>(dl_result)<br><span class="hljs-built_in">print</span>(dl_result.shape)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[ <span class="hljs-number">1.9604e+00</span>,  <span class="hljs-number">3.9288e+01</span>, -<span class="hljs-number">5.2422e+01</span>,  ...,  <span class="hljs-number">2.1041e-01</span>,<br>          -<span class="hljs-number">5.5063e+01</span>,  <span class="hljs-number">1.5233e-01</span>],<br>         [ <span class="hljs-number">1.0135e-01</span>, -<span class="hljs-number">3.7779e-01</span>,  <span class="hljs-number">6.5491e+01</span>,  ...,  <span class="hljs-number">2.8062e+01</span>,<br>          -<span class="hljs-number">3.7780e+01</span>, -<span class="hljs-number">3.9577e+01</span>],<br>         [ <span class="hljs-number">1.9526e+01</span>, -<span class="hljs-number">2.5741e+01</span>,  <span class="hljs-number">2.6926e-01</span>,  ..., -<span class="hljs-number">1.5316e+01</span>,<br>           <span class="hljs-number">1.4543e+00</span>,  <span class="hljs-number">2.7714e+00</span>],<br>         [-<span class="hljs-number">2.1528e+01</span>,  <span class="hljs-number">2.0141e+01</span>,  <span class="hljs-number">2.1999e+01</span>,  ...,  <span class="hljs-number">2.2099e+00</span>,<br>          -<span class="hljs-number">1.7267e+01</span>, -<span class="hljs-number">1.6687e+01</span>]],<br><br>        [[ <span class="hljs-number">6.7259e+00</span>, -<span class="hljs-number">2.6918e+01</span>,  <span class="hljs-number">1.1807e+01</span>,  ..., -<span class="hljs-number">3.6453e+01</span>,<br>          -<span class="hljs-number">2.9231e+01</span>,  <span class="hljs-number">1.1288e+01</span>],<br>         [ <span class="hljs-number">7.7484e+01</span>, -<span class="hljs-number">5.0572e-01</span>, -<span class="hljs-number">1.3096e+01</span>,  ...,  <span class="hljs-number">3.6302e-01</span>,<br>           <span class="hljs-number">1.9907e+01</span>, -<span class="hljs-number">1.2160e+00</span>],<br>         [ <span class="hljs-number">2.6703e+01</span>,  <span class="hljs-number">4.4737e+01</span>, -<span class="hljs-number">3.1590e+01</span>,  ...,  <span class="hljs-number">4.1540e-03</span>,<br>           <span class="hljs-number">5.2587e+00</span>,  <span class="hljs-number">5.2382e+00</span>],<br>         [ <span class="hljs-number">4.7435e+01</span>, -<span class="hljs-number">3.7599e-01</span>,  <span class="hljs-number">5.0898e+01</span>,  ...,  <span class="hljs-number">5.6361e+00</span>,<br>           <span class="hljs-number">3.5891e+01</span>,  <span class="hljs-number">1.5697e+01</span>]]], grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<p><code>解码器层总结</code>：</p>
<ul>
<li>学习了解码器层的作用:<ul>
<li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了解码器层的类: DecoderLayer<ul>
<li>类的初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q&#x3D;K&#x3D;V，第三个是src_attn，多头注意力对象，这里Q!&#x3D;K&#x3D;V， 第四个是前馈全连接层对象，最后就是droupout置0比率.</li>
<li>forward函数的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</li>
<li>最终输出了由编码器输入和目标数据一同作用的特征提取结果.</li>
</ul>
</li>
</ul>
<h3 id="2-4-2-解码器"><a href="#2-4-2-解码器" class="headerlink" title="2.4.2 解码器"></a>2.4.2 解码器</h3><ul>
<li>学习目标:<ul>
<li>了解解码器的作用.</li>
<li>掌握解码器的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>解码器的作用:<ul>
<li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>解码器的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用类Decoder来实现解码器</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, layer, N</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(Decoder, self).__init__()<br>        <span class="hljs-comment"># 首先使用clones方法克隆了N个layer，然后实例化了一个规范化层. </span><br>        <span class="hljs-comment"># 因为数据走过了所有的解码器层后最后要做规范化处理. </span><br>        self.layers = clones(layer, N)<br>        self.norm = LayerNorm(layer.size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, memory, source_mask, target_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，</span><br><span class="hljs-string">           source_mask, target_mask代表源数据和目标数据的掩码张量&quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，</span><br>        <span class="hljs-comment"># 得出最后的结果，再进行一次规范化返回即可. </span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>            x = layer(x, memory, source_mask, target_mask)<br>        <span class="hljs-keyword">return</span> self.norm(x)<br></code></pre></td></tr></table></figure>

<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 分别是解码器层layer和解码器层的个数N</span><br>size = <span class="hljs-number">512</span><br>d_model = <span class="hljs-number">512</span><br>head = <span class="hljs-number">8</span><br>d_ff = <span class="hljs-number">64</span><br>dropout = <span class="hljs-number">0.2</span><br>c = copy.deepcopy<br>attn = MultiHeadedAttention(head, d_model)<br>ff = PositionwiseFeedForward(d_model, d_ff, dropout)<br>layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)<br>N = <span class="hljs-number">8</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入参数与解码器层的输入参数相同</span><br>x = pe_result<br>memory = en_result<br>mask = Variable(torch.zeros(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br>source_mask = target_mask = mask<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">de = Decoder(layer, N)<br>de_result = de(x, memory, source_mask, target_mask)<br><span class="hljs-built_in">print</span>(de_result)<br><span class="hljs-built_in">print</span>(de_result.shape)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[ <span class="hljs-number">0.9898</span>, -<span class="hljs-number">0.3216</span>, -<span class="hljs-number">1.2439</span>,  ...,  <span class="hljs-number">0.7427</span>, -<span class="hljs-number">0.0717</span>, -<span class="hljs-number">0.0814</span>],<br>         [-<span class="hljs-number">0.7432</span>,  <span class="hljs-number">0.6985</span>,  <span class="hljs-number">1.5551</span>,  ...,  <span class="hljs-number">0.5232</span>, -<span class="hljs-number">0.5685</span>,  <span class="hljs-number">1.3387</span>],<br>         [ <span class="hljs-number">0.2149</span>,  <span class="hljs-number">0.5274</span>, -<span class="hljs-number">1.6414</span>,  ...,  <span class="hljs-number">0.7476</span>,  <span class="hljs-number">0.5082</span>, -<span class="hljs-number">3.0132</span>],<br>         [ <span class="hljs-number">0.4408</span>,  <span class="hljs-number">0.9416</span>,  <span class="hljs-number">0.4522</span>,  ..., -<span class="hljs-number">0.1506</span>,  <span class="hljs-number">1.5591</span>, -<span class="hljs-number">0.6453</span>]],<br><br>        [[-<span class="hljs-number">0.9027</span>,  <span class="hljs-number">0.5874</span>,  <span class="hljs-number">0.6981</span>,  ...,  <span class="hljs-number">2.2899</span>,  <span class="hljs-number">0.2933</span>, -<span class="hljs-number">0.7508</span>],<br>         [ <span class="hljs-number">1.2246</span>, -<span class="hljs-number">1.0856</span>, -<span class="hljs-number">0.2497</span>,  ..., -<span class="hljs-number">1.2377</span>,  <span class="hljs-number">0.0847</span>, -<span class="hljs-number">0.0221</span>],<br>         [ <span class="hljs-number">3.4012</span>, -<span class="hljs-number">0.4181</span>, -<span class="hljs-number">2.0968</span>,  ..., -<span class="hljs-number">1.5427</span>,  <span class="hljs-number">0.1090</span>, -<span class="hljs-number">0.3882</span>],<br>         [-<span class="hljs-number">0.1050</span>, -<span class="hljs-number">0.5140</span>, -<span class="hljs-number">0.6494</span>,  ..., -<span class="hljs-number">0.4358</span>, -<span class="hljs-number">1.2173</span>,  <span class="hljs-number">0.4161</span>]]],<br>       grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<p><strong>解码器总结：</strong></p>
<ul>
<li>学习了解码器的作用:<ul>
<li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了解码器的类: Decoder<ul>
<li>类的初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N.</li>
<li>forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，src_mask, tgt_mask代表源数据和目标数据的掩码张量.</li>
<li>输出解码过程的最终特征表示.</li>
</ul>
</li>
</ul>
<h2 id="2-5-输出部分实现"><a href="#2-5-输出部分实现" class="headerlink" title="2.5 输出部分实现"></a>2.5 输出部分实现</h2><ul>
<li><p>学习目标</p>
<ul>
<li><p>了解线性层和softmax的作用.</p>
</li>
<li><p>掌握线性层和softmax的实现过程.</p>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>输出部分包含:<ul>
<li>线性层</li>
<li>softmax层</li>
</ul>
</li>
</ul>
<p><img src="https://pic3.zhimg.com/80/v2-4b2d21a7199c6f261c9caca82e213b96_1440w.webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<ul>
<li>线性层的作用<ul>
<li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>softmax层的作用</li>
<li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li>
</ul>
<hr>
<ul>
<li>线性层和softmax层的代码分析:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># nn.functional工具包装载了网络层中那些只进行计算, 而没有参数的层</span><br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-comment"># 将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构</span><br><span class="hljs-comment"># 因此把类的名字叫做Generator, 生成器类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Generator</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, vocab_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(Generator, self).__init__()<br>        <span class="hljs-comment"># 首先就是使用nn中的预定义线性层进行实例化, 得到一个对象self.project等待使用, </span><br>        <span class="hljs-comment"># 这个线性层的参数有两个, 就是初始化函数传进来的两个参数: d_model, vocab_size</span><br>        self.project = nn.Linear(d_model, vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;前向逻辑函数中输入是上一层的输出张量x&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 在函数中, 首先使用上一步得到的self.project对x进行线性变化, </span><br>        <span class="hljs-comment"># 然后使用F中已经实现的log_softmax进行的softmax处理.</span><br>        <span class="hljs-comment"># 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.</span><br>        <span class="hljs-comment"># log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数, </span><br>        <span class="hljs-comment"># 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.</span><br>        <span class="hljs-keyword">return</span> F.log_softmax(self.project(x), dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>nn.Linear演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">30</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">input</span> = torch.randn(<span class="hljs-number">128</span>, <span class="hljs-number">20</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>output = m(<span class="hljs-built_in">input</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(output.size())<br>torch.Size([<span class="hljs-number">128</span>, <span class="hljs-number">30</span>])<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>实例化参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 词嵌入维度是512维</span><br>d_model = <span class="hljs-number">512</span><br><br><span class="hljs-comment"># 词表大小是1000</span><br>vocab_size = <span class="hljs-number">1000</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入x是上一层网络的输出, 我们使用来自解码器层的输出</span><br>x = de_result<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">gen = Generator(d_model, vocab_size)<br>gen_result = gen(x)<br><span class="hljs-built_in">print</span>(gen_result)<br><span class="hljs-built_in">print</span>(gen_result.shape)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[-<span class="hljs-number">7.8098</span>, -<span class="hljs-number">7.5260</span>, -<span class="hljs-number">6.9244</span>,  ..., -<span class="hljs-number">7.6340</span>, -<span class="hljs-number">6.9026</span>, -<span class="hljs-number">7.5232</span>],<br>         [-<span class="hljs-number">6.9093</span>, -<span class="hljs-number">7.3295</span>, -<span class="hljs-number">7.2972</span>,  ..., -<span class="hljs-number">6.6221</span>, -<span class="hljs-number">7.2268</span>, -<span class="hljs-number">7.0772</span>],<br>         [-<span class="hljs-number">7.0263</span>, -<span class="hljs-number">7.2229</span>, -<span class="hljs-number">7.8533</span>,  ..., -<span class="hljs-number">6.7307</span>, -<span class="hljs-number">6.9294</span>, -<span class="hljs-number">7.3042</span>],<br>         [-<span class="hljs-number">6.5045</span>, -<span class="hljs-number">6.0504</span>, -<span class="hljs-number">6.6241</span>,  ..., -<span class="hljs-number">5.9063</span>, -<span class="hljs-number">6.5361</span>, -<span class="hljs-number">7.1484</span>]],<br><br>        [[-<span class="hljs-number">7.1651</span>, -<span class="hljs-number">6.0224</span>, -<span class="hljs-number">7.4931</span>,  ..., -<span class="hljs-number">7.9565</span>, -<span class="hljs-number">8.0460</span>, -<span class="hljs-number">6.6490</span>],<br>         [-<span class="hljs-number">6.3779</span>, -<span class="hljs-number">7.6133</span>, -<span class="hljs-number">8.3572</span>,  ..., -<span class="hljs-number">6.6565</span>, -<span class="hljs-number">7.1867</span>, -<span class="hljs-number">6.5112</span>],<br>         [-<span class="hljs-number">6.4914</span>, -<span class="hljs-number">6.9289</span>, -<span class="hljs-number">6.2634</span>,  ..., -<span class="hljs-number">6.2471</span>, -<span class="hljs-number">7.5348</span>, -<span class="hljs-number">6.8541</span>],<br>         [-<span class="hljs-number">6.8651</span>, -<span class="hljs-number">7.0460</span>, -<span class="hljs-number">7.6239</span>,  ..., -<span class="hljs-number">7.1411</span>, -<span class="hljs-number">6.5496</span>, -<span class="hljs-number">7.3749</span>]]],<br>       grad_fn=&lt;LogSoftmaxBackward&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1000</span>])<br></code></pre></td></tr></table></figure>

<ul>
<li><p>小节总结</p>
<ul>
<li>学习了输出部分包含:<ul>
<li>线性层</li>
<li>softmax层</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>线性层的作用:<ul>
<li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>softmax层的作用:</p>
<ul>
<li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>学习并实现了线性层和softmax层的类: Generator</p>
<ul>
<li>初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小.</li>
<li>forward函数接受上一层的输出.</li>
<li>最终获得经过线性层和softmax层处理的结果.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-6-模型的构建"><a href="#2-6-模型的构建" class="headerlink" title="2.6 模型的构建"></a>2.6 模型的构建</h2><ul>
<li>学习目标<ul>
<li>掌握编码器-解码器结构的实现过程.</li>
<li>掌握Transformer模型的构建过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>通过上面的小节, 我们已经完成了所有组成部分的实现, 接下来就来实现完整的编码器-解码器结构.</li>
</ul>
<hr>
<ul>
<li>Transformer总体架构图:</li>
</ul>
<img src="https://article.biliimg.com/bfs/article/e29cac3e97a731b009805f5fc7842ffa155712160.png" srcset="/img/loading.gif" lazyload style="zoom:80%;" />

<h3 id="编码器-解码器结构的代码实现"><a href="#编码器-解码器结构的代码实现" class="headerlink" title="编码器-解码器结构的代码实现"></a>编码器-解码器结构的代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用EncoderDecoder类来实现编码器-解码器结构</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderDecoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, encoder, decoder, source_embed, target_embed, generator</span>):<br>        <span class="hljs-comment"># encoder：代表编码器对象</span><br>        <span class="hljs-comment"># decoder：代表解码器对象</span><br>        <span class="hljs-comment"># source_embed：代表源数据的嵌入函数</span><br>        <span class="hljs-comment"># target_embed：代表目标数据的嵌入函数</span><br>        <span class="hljs-comment"># generator：代表输出部分类别生成器对象</span><br>        <span class="hljs-string">&quot;&quot;&quot;初始化函数中有5个参数, 分别是编码器对象, 解码器对象, </span><br><span class="hljs-string">           源数据嵌入函数, 目标数据嵌入函数,  以及输出部分的类别生成器对象</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(EncoderDecoder, self).__init__()<br>        <span class="hljs-comment"># 将参数传入到类中</span><br>        self.encoder = encoder<br>        self.decoder = decoder<br>        self.src_embed = source_embed<br>        self.tgt_embed = target_embed<br>        self.generator = generator<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, source, target, source_mask, target_mask</span>):<br>        <span class="hljs-comment"># source：代表源数据</span><br>        <span class="hljs-comment"># target：代表目标数据</span><br>        <span class="hljs-comment"># source_mask：代表源数据的掩码张量</span><br>        <span class="hljs-comment"># target_mask：代表目标数据的掩码张量</span><br>        <span class="hljs-string">&quot;&quot;&quot;在forward函数中，有四个参数, source代表源数据, target代表目标数据, </span><br><span class="hljs-string">           source_mask和target_mask代表对应的掩码张量&quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 在函数中, 将source, source_mask传入编码函数, 得到结果后,</span><br>        <span class="hljs-comment"># 与source_mask，target，和target_mask一同传给解码函数.</span><br>        <span class="hljs-keyword">return</span> self.generator(self.decode(self.encode(source, source_mask), source_mask,<br>                            target, target_mask))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, source, source_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;编码函数, 以source和source_mask为参数&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 使用src_embed对source做处理, 然后和source_mask一起传给self.encoder</span><br>        <span class="hljs-keyword">return</span> self.encoder(self.src_embed(source), source_mask)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, memory, source_mask, target, target_mask</span>):<br>        <span class="hljs-comment"># memory：代表经历编码器编码后的输出张量</span><br>        <span class="hljs-string">&quot;&quot;&quot;解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 使用tgt_embed对target做处理, 然后和source_mask, target_mask, memory一起传给self.decoder</span><br>        <span class="hljs-keyword">return</span> self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>实例化参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab_size = <span class="hljs-number">1000</span><br>d_model = <span class="hljs-number">512</span><br>encoder = en<br>decoder = de<br>source_embed = nn.Embedding(vocab_size, d_model)<br>target_embed = nn.Embedding(vocab_size, d_model)<br>generator = gen<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设源数据与目标数据相同, 实际中并不相同</span><br>source = target = Variable(torch.LongTensor([[<span class="hljs-number">100</span>, <span class="hljs-number">2</span>, <span class="hljs-number">421</span>, <span class="hljs-number">508</span>], [<span class="hljs-number">491</span>, <span class="hljs-number">998</span>, <span class="hljs-number">1</span>, <span class="hljs-number">221</span>]]))<br><br><span class="hljs-comment"># 假设src_mask与tgt_mask相同，实际中并不相同</span><br>source_mask = target_mask = Variable(torch.zeros(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">ed = EncoderDecoder(encoder, decoder, source_embed, target_embed, generator)<br>ed_result = ed(source, target, source_mask, target_mask)<br><span class="hljs-built_in">print</span>(ed_result)<br><span class="hljs-built_in">print</span>(ed_result.shape)<br></code></pre></td></tr></table></figure>

<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[[ <span class="hljs-number">0.2102</span>, -<span class="hljs-number">0.0826</span>, -<span class="hljs-number">0.0550</span>,  ...,  <span class="hljs-number">1.5555</span>,  <span class="hljs-number">1.3025</span>, -<span class="hljs-number">0.6296</span>],<br>         [ <span class="hljs-number">0.8270</span>, -<span class="hljs-number">0.5372</span>, -<span class="hljs-number">0.9559</span>,  ...,  <span class="hljs-number">0.3665</span>,  <span class="hljs-number">0.4338</span>, -<span class="hljs-number">0.7505</span>],<br>         [ <span class="hljs-number">0.4956</span>, -<span class="hljs-number">0.5133</span>, -<span class="hljs-number">0.9323</span>,  ...,  <span class="hljs-number">1.0773</span>,  <span class="hljs-number">1.1913</span>, -<span class="hljs-number">0.6240</span>],<br>         [ <span class="hljs-number">0.5770</span>, -<span class="hljs-number">0.6258</span>, -<span class="hljs-number">0.4833</span>,  ...,  <span class="hljs-number">0.1171</span>,  <span class="hljs-number">1.0069</span>, -<span class="hljs-number">1.9030</span>]],<br><br>        [[-<span class="hljs-number">0.4355</span>, -<span class="hljs-number">1.7115</span>, -<span class="hljs-number">1.5685</span>,  ..., -<span class="hljs-number">0.6941</span>, -<span class="hljs-number">0.1878</span>, -<span class="hljs-number">0.1137</span>],<br>         [-<span class="hljs-number">0.8867</span>, -<span class="hljs-number">1.2207</span>, -<span class="hljs-number">1.4151</span>,  ..., -<span class="hljs-number">0.9618</span>,  <span class="hljs-number">0.1722</span>, -<span class="hljs-number">0.9562</span>],<br>         [-<span class="hljs-number">0.0946</span>, -<span class="hljs-number">0.9012</span>, -<span class="hljs-number">1.6388</span>,  ..., -<span class="hljs-number">0.2604</span>, -<span class="hljs-number">0.3357</span>, -<span class="hljs-number">0.6436</span>],<br>         [-<span class="hljs-number">1.1204</span>, -<span class="hljs-number">1.4481</span>, -<span class="hljs-number">1.5888</span>,  ..., -<span class="hljs-number">0.8816</span>, -<span class="hljs-number">0.6497</span>,  <span class="hljs-number">0.0606</span>]]],<br>       grad_fn=&lt;AddBackward0&gt;)<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>接着将基于以上结构构建用于训练的模型.</li>
</ul>
<hr>
<ul>
<li>Tansformer模型构建过程的代码分析</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_model</span>(<span class="hljs-params">source_vocab, target_vocab, N=<span class="hljs-number">6</span>, </span><br><span class="hljs-params">               d_model=<span class="hljs-number">512</span>, d_ff=<span class="hljs-number">2048</span>, head=<span class="hljs-number">8</span>, dropout=<span class="hljs-number">0.1</span></span>):<br>    <span class="hljs-comment">#source_voc ab：代表源数据的词汇总数</span><br>    <span class="hljs-comment">#target_voc ab：代表目标数据的词汇总数</span><br>    <span class="hljs-comment">#N：代表编码器和解码器堆叠的层数</span><br>    <span class="hljs-comment">#d_model：代表词嵌入的维度</span><br>    <span class="hljs-comment">#d_ff：代表前馈全连接层中变换矩阵的维度</span><br>    <span class="hljs-comment">#head：多头注意力机制中的头数</span><br>    <span class="hljs-comment">#dropout：指置零的比率</span><br><br>    <span class="hljs-string">&quot;&quot;&quot;该函数用来构建模型, 有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，</span><br><span class="hljs-string">       编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，</span><br><span class="hljs-string">       多头注意力结构中的多头数，以及置零比率dropout.&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># 首先得到一个深度拷贝命令，接下来很多结构都需要进行深度拷贝，</span><br>    <span class="hljs-comment"># 来保证他们彼此之间相互独立，不受干扰.</span><br>    c = copy.deepcopy<br><br>    <span class="hljs-comment"># 实例化了多头注意力类，得到对象attn</span><br>    attn = MultiHeadedAttention(head, d_model)<br><br>    <span class="hljs-comment"># 然后实例化前馈全连接类，得到对象ff </span><br>    ff = PositionwiseFeedForward(d_model, d_ff, dropout)<br><br>    <span class="hljs-comment"># 实例化位置编码类，得到对象position</span><br>    position = PositionalEncoding(d_model, dropout)<br><br>    <span class="hljs-comment"># 根据结构图, 最外层是EncoderDecoder，在EncoderDecoder中，</span><br>    <span class="hljs-comment"># 分别是编码器层，解码器层，源数据Embedding层和位置编码组成的有序结构，</span><br>    <span class="hljs-comment"># 目标数据Embedding层和位置编码组成的有序结构，以及类别生成器层. </span><br>    <span class="hljs-comment"># 在编码器层中有attention子层以及前馈全连接子层，</span><br>    <span class="hljs-comment"># 在解码器层中有两个attention子层以及前馈全连接层.</span><br>    <br>    <span class="hljs-comment">#实例化模型model，利用的是Encoder Decoder类</span><br>	<span class="hljs-comment">#编码器的结构里面有2个子层，attention层和前馈全连接层</span><br>	<span class="hljs-comment">#解码器的结构中有3个子层，两个attention层和前馈全连接层</span><br><br>    model = EncoderDecoder(<br>        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),<br>        Decoder(DecoderLayer(d_model, c(attn), c(attn), <br>                             c(ff), dropout), N),<br>        nn.Sequential(Embeddings(d_model, source_vocab), c(position)),<br>        nn.Sequential(Embeddings(d_model, target_vocab), c(position)),<br>        Generator(d_model, target_vocab))<br><br>    <span class="hljs-comment"># 模型结构完成后，接下来就是初始化模型中的参数，比如线性层中的变换矩阵</span><br>    <span class="hljs-comment"># 这里一但判断参数的维度大于1，则会将其初始化成一个服从均匀分布的矩阵，</span><br>    <span class="hljs-comment">#初始化整个模型中的参数，判断参数的维度大于1，将矩阵初始化成一个服从均匀分布的矩阵</span><br><br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters():<br>        <span class="hljs-keyword">if</span> p.dim() &gt; <span class="hljs-number">1</span>:<br>            nn.init.xavier_uniform(p)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure>

<ul>
<li>nn.init.xavier_uniform演示:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 结果服从均匀分布U(-a, a)</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>w = nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(<span class="hljs-string">&#x27;relu&#x27;</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>w<br>tensor([[-<span class="hljs-number">0.7742</span>,  <span class="hljs-number">0.5413</span>,  <span class="hljs-number">0.5478</span>, -<span class="hljs-number">0.4806</span>, -<span class="hljs-number">0.2555</span>],<br>        [-<span class="hljs-number">0.8358</span>,  <span class="hljs-number">0.4673</span>,  <span class="hljs-number">0.3012</span>,  <span class="hljs-number">0.3882</span>, -<span class="hljs-number">0.6375</span>],<br>        [ <span class="hljs-number">0.4622</span>, -<span class="hljs-number">0.0794</span>,  <span class="hljs-number">0.1851</span>,  <span class="hljs-number">0.8462</span>, -<span class="hljs-number">0.3591</span>]])<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">source_vocab = <span class="hljs-number">11</span><br>target_vocab = <span class="hljs-number">11</span> <br>N = <span class="hljs-number">6</span><br><span class="hljs-comment"># 其他参数都使用默认值 </span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    res = make_model(source_vocab, target_vocab, N)<br>    <span class="hljs-built_in">print</span>(res)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 根据Transformer结构图构建的最终模型结构</span><br>EncoderDecoder(<br>  (encoder): Encoder(<br>    (layers): ModuleList(<br>      (<span class="hljs-number">0</span>): EncoderLayer(<br>        (self_attn): MultiHeadedAttention(<br>          (linears): ModuleList(<br>            (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">1</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>          )<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (feed_forward): PositionwiseFeedForward(<br>          (w_1): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">2048</span>)<br>          (w_2): Linear(in_features=<span class="hljs-number">2048</span>, out_features=<span class="hljs-number">512</span>)<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (sublayer): ModuleList(<br>          (<span class="hljs-number">0</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>          (<span class="hljs-number">1</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>        )<br>      )<br>      (<span class="hljs-number">1</span>): EncoderLayer(<br>        (self_attn): MultiHeadedAttention(<br>          (linears): ModuleList(<br>            (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">1</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>          )<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (feed_forward): PositionwiseFeedForward(<br>          (w_1): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">2048</span>)<br>          (w_2): Linear(in_features=<span class="hljs-number">2048</span>, out_features=<span class="hljs-number">512</span>)<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (sublayer): ModuleList(<br>          (<span class="hljs-number">0</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>          (<span class="hljs-number">1</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>        )<br>      )<br>    )<br>    (norm): LayerNorm(<br>    )<br>  )<br>  (decoder): Decoder(<br>    (layers): ModuleList(<br>      (<span class="hljs-number">0</span>): DecoderLayer(<br>        (self_attn): MultiHeadedAttention(<br>          (linears): ModuleList(<br>            (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">1</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>          )<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (src_attn): MultiHeadedAttention(<br>          (linears): ModuleList(<br>            (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">1</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>          )<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (feed_forward): PositionwiseFeedForward(<br>          (w_1): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">2048</span>)<br>          (w_2): Linear(in_features=<span class="hljs-number">2048</span>, out_features=<span class="hljs-number">512</span>)<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (sublayer): ModuleList(<br>          (<span class="hljs-number">0</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>          (<span class="hljs-number">1</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>          (<span class="hljs-number">2</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>        )<br>      )<br>      (<span class="hljs-number">1</span>): DecoderLayer(<br>        (self_attn): MultiHeadedAttention(<br>          (linears): ModuleList(<br>            (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">1</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>          )<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (src_attn): MultiHeadedAttention(<br>          (linears): ModuleList(<br>            (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">1</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>            (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>)<br>          )<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (feed_forward): PositionwiseFeedForward(<br>          (w_1): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">2048</span>)<br>          (w_2): Linear(in_features=<span class="hljs-number">2048</span>, out_features=<span class="hljs-number">512</span>)<br>          (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>        )<br>        (sublayer): ModuleList(<br>          (<span class="hljs-number">0</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>          (<span class="hljs-number">1</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>          (<span class="hljs-number">2</span>): SublayerConnection(<br>            (norm): LayerNorm(<br>            )<br>            (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>          )<br>        )<br>      )<br>    )<br>    (norm): LayerNorm(<br>    )<br>  )<br>  (src_embed): Sequential(<br>    (<span class="hljs-number">0</span>): Embeddings(<br>      (lut): Embedding(<span class="hljs-number">11</span>, <span class="hljs-number">512</span>)<br>    )<br>    (<span class="hljs-number">1</span>): PositionalEncoding(<br>      (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>    )<br>  )<br>  (tgt_embed): Sequential(<br>    (<span class="hljs-number">0</span>): Embeddings(<br>      (lut): Embedding(<span class="hljs-number">11</span>, <span class="hljs-number">512</span>)<br>    )<br>    (<span class="hljs-number">1</span>): PositionalEncoding(<br>      (dropout): Dropout(p=<span class="hljs-number">0.1</span>)<br>    )<br>  )<br>  (generator): Generator(<br>    (proj): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">11</span>)<br>  )<br>)<br></code></pre></td></tr></table></figure>

<p><strong>小节总结</strong></p>
<ul>
<li>学习并实现了编码器-解码器结构的类: EncoderDecoder<ul>
<li>类的初始化函数传入5个参数, 分别是编码器对象, 解码器对象, 源数据嵌入函数, 目标数据嵌入函数, 以及输出部分的类别生成器对象.</li>
<li>类中共实现三个函数, forward, encode, decode</li>
<li>forward是主要逻辑函数, 有四个参数, source代表源数据, target代表目标数据, source_mask和target_mask代表对应的掩码张量.</li>
<li>encode是编码函数, 以source和source_mask为参数.</li>
<li>decode是解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了模型构建函数: make_model<ul>
<li>有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，多头注意力结构中的多头数，以及置零比率dropout.</li>
</ul>
</li>
</ul>
<h2 id="2-7-模型基本测试运行-P44"><a href="#2-7-模型基本测试运行-P44" class="headerlink" title="2.7 模型基本测试运行 P44"></a>2.7 模型基本测试运行 P44</h2><ul>
<li><p>学习目标</p>
<ul>
<li><p>了解Transformer模型基本测试的copy任务.</p>
</li>
<li><p>掌握实现copy任务的四步曲.</p>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>copy任务介绍:</p>
<ul>
<li>任务描述: 针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</li>
<li>任务意义: copy任务在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>使用copy任务进行模型基本测试的四步曲<ul>
<li>第一步: 构建数据集生成器</li>
<li>第二步: 获得Transformer模型及其优化器和损失函数</li>
<li>第三步: 运行模型进行训练和评估</li>
<li>第四步: 使用模型进行贪婪解码</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>第一步: 构建数据集生成器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入工具包Batch, 它能够对原始样本数据生成对应批次的掩码张量</span><br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> Batch  <br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> get_std_opt<br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> LabelSmoothing<br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> SimpleLossCompute<br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> run_epoch<br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> greedy_decode<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generator</span>(<span class="hljs-params">V, batch, num_batch</span>):<br>    <span class="hljs-comment"># V：随机生成数据的最大值+1</span><br>	<span class="hljs-comment"># batch_size：每次输送给模型的样本数量，经历这些样本训练后进行一次参数的更新</span><br>	<span class="hljs-comment"># num_batch：一共输送模型多少轮数据</span><br><br>    <span class="hljs-string">&quot;&quot;&quot;该函数用于随机生成copy任务的数据, 它的三个输入参数是V: 随机生成数字的最大值+1, </span><br><span class="hljs-string">       batch: 每次输送给模型更新一次参数的数据量, num_batch: 一共输送num_batch次完成一轮</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 使用for循环遍历nbatches</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batch):<br>        <span class="hljs-comment"># 使用num py中的random.rand in t（）来随机生成[1，V)</span><br>		<span class="hljs-comment"># 分布的形状(batch，10）</span><br><br>        <span class="hljs-comment"># 在循环中使用np的random.randint方法随机生成[1, V)的整数, </span><br>        <span class="hljs-comment"># 分布在(batch, 10)形状的矩阵中, 然后再把numpy形式转换称torch中的tensor.</span><br>        data = torch.from_numpy(np.random.randint(<span class="hljs-number">1</span>, V, size=(batch, <span class="hljs-number">10</span>)))<br><br>        <span class="hljs-comment"># 接着使数据矩阵中的第一列数字都为1, 这一列也就成为了起始标志列, </span><br>        <span class="hljs-comment"># 当解码器进行第一次解码的时候, 会使用起始标志列作为输入.</span><br>        data[:, <span class="hljs-number">0</span>] = <span class="hljs-number">1</span><br><br>        <span class="hljs-comment"># 因为是copy任务, 所有source与target是完全相同的, 且数据样本作用变量不需要求梯度</span><br>        <span class="hljs-comment"># 因此requires_grad设置为False</span><br>        source = Variable(data, requires_grad=<span class="hljs-literal">False</span>)<br>        target = Variable(data, requires_grad=<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-comment"># 使用Batch对source和target进行对应批次的掩码张量生成, 最后使用yield返回</span><br>        <span class="hljs-keyword">yield</span> Batch(source, target)<br></code></pre></td></tr></table></figure>

<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将生成0-10的整数</span><br>V = <span class="hljs-number">11</span><br><br><span class="hljs-comment"># 每次喂给模型20个数据进行参数更新</span><br>batch = <span class="hljs-number">20</span> <br><br><span class="hljs-comment"># 连续喂30次完成全部数据的遍历, 也就是1轮</span><br>num_batch = <span class="hljs-number">30</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>调用:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    res = data_generator(V, batch, num_batch)<br>    <span class="hljs-built_in">print</span>(res)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 会得到一个数据生成器(生成器对象)</span><br>&lt;generator <span class="hljs-built_in">object</span> data_gen at <span class="hljs-number">0x10c053e08</span>&gt;<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>第二步: 获得Transformer模型及其优化器和损失函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入优化器工具包get_std_opt, 该工具用于获得标准的针对Transformer模型的优化器 </span><br><span class="hljs-comment"># 该标准优化器基于Adam优化器, 使其对序列到序列的任务更有效.</span><br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> get_std_opt<br><br><span class="hljs-comment"># 导入标签平滑工具包, 该工具用于标签平滑, 标签平滑的作用就是小幅度的改变原有标签值的值域</span><br><span class="hljs-comment"># 因为在理论上即使是人工的标注数据也可能并非完全正确, 会受到一些外界因素的影响而产生一些微小的偏差</span><br><span class="hljs-comment"># 因此使用标签平滑来弥补这种偏差, 减少模型对某一条规律的绝对认知, 以防止过拟合. 通过下面示例了解更多.</span><br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> LabelSmoothing<br><br><span class="hljs-comment"># 导入损失计算工具包, 该工具能够使用标签平滑后的结果进行损失的计算, </span><br><span class="hljs-comment"># 损失的计算方法可以认为是交叉熵损失函数.</span><br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> SimpleLossCompute<br><br><span class="hljs-comment"># 使用make_model获得model</span><br>model = make_model(V, V, N=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 使用get_std_opt获得模型优化器</span><br>model_optimizer = get_std_opt(model)<br><br><span class="hljs-comment"># 使用LabelSmoothing获得标签平滑对象</span><br>criterion = LabelSmoothing(size=V, padding_idx=<span class="hljs-number">0</span>, smoothing=<span class="hljs-number">0.0</span>)<br><br><span class="hljs-comment"># 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法</span><br>loss = SimpleLossCompute(model.generator, criterion, model_optimizer)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>标签平滑示例:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> LabelSmoothing<br><br><span class="hljs-comment"># 使用LabelSmoothing实例化一个crit对象.</span><br><span class="hljs-comment"># 第一个参数size代表目标数据的词汇总数, 也是模型最后一层得到张量的最后一维大小</span><br><span class="hljs-comment"># 这里是5说明目标词汇总数是5个. 第二个参数padding_idx表示要将那些tensor中的数字</span><br><span class="hljs-comment"># 替换成0, 一般padding_idx=0表示不进行替换. 第三个参数smoothing, 表示标签的平滑程度</span><br><span class="hljs-comment"># 如原来标签的表示值为1, 则平滑后它的值域变为[1-smoothing, 1+smoothing].</span><br>crit = LabelSmoothing(size=<span class="hljs-number">5</span>, padding_idx=<span class="hljs-number">0</span>, smoothing=<span class="hljs-number">0.5</span>)<br><br><span class="hljs-comment"># 假定一个任意的模型最后输出预测结果和真实结果</span><br>predict = Variable(torch.FloatTensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0</span>],<br>                             [<span class="hljs-number">0</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0</span>], <br>                             [<span class="hljs-number">0</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0</span>]]))<br><br><span class="hljs-comment"># 标签的表示值是0，1，2</span><br>target = Variable(torch.LongTensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]))<br><br><span class="hljs-comment"># 将predict, target传入到对象中</span><br>crit(predict, target)<br><br><span class="hljs-comment"># 绘制标签平滑图像</span><br>plt.imshow(crit.true_dist)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>标签平滑图像:</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202311052258109.png" srcset="/img/loading.gif" lazyload alt="image-20231105223915947"></p>
<ul>
<li><p>标签平滑图像分析:</p>
<ul>
<li>我们目光集中在<code>黄色小方块</code>上, 它相对于横坐标横跨的值域就是标签平滑后的正向平滑值域, 我们可以看到大致是从0.5到2.5.</li>
<li>它相对于纵坐标横跨的值域就是标签平滑后的负向平滑值域, 我们可以看到大致是从-0.5到1.5, 总的值域空间由原来的[0, 2]变成了[-0.5, 2.5].</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>第三步: 运行模型进行训练和评估</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入模型单轮训练工具包run_epoch, 该工具将对模型使用给定的损失函数计算方法进行单轮参数更新.</span><br><span class="hljs-comment"># 并打印每轮参数更新的损失结果.</span><br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> run_epoch<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">model, loss, epochs=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;模型训练函数, 共有三个参数, model代表将要进行训练的模型</span><br><span class="hljs-string">       loss代表使用的损失计算方法, epochs代表模型训练的轮数&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># 遍历轮数</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-comment"># 模型使用训练模式, 所有参数将被更新</span><br>        model.train()<br>        <span class="hljs-comment"># 训练时, batch_size是20</span><br>        run_epoch(data_generator(V, <span class="hljs-number">8</span>, <span class="hljs-number">20</span>), model, loss)<br><br>        <span class="hljs-comment"># 模型使用评估模式, 参数将不会变化 </span><br>        model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-comment"># 评估时, batch_size是5</span><br>        run_epoch(data_generator(V, <span class="hljs-number">8</span>, <span class="hljs-number">5</span>), model, loss)<br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输入参数:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 进行10轮训练</span><br>epochs = <span class="hljs-number">10</span><br><br><span class="hljs-comment"># model和loss都是来自上一步的结果</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">3.315704</span> Tokens per Sec: <span class="hljs-number">309.740843</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">2.602743</span> Tokens per Sec: <span class="hljs-number">393.885743</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">2.563469</span> Tokens per Sec: <span class="hljs-number">347.746994</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">2.065951</span> Tokens per Sec: <span class="hljs-number">422.632783</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">2.218468</span> Tokens per Sec: <span class="hljs-number">346.982987</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.771149</span> Tokens per Sec: <span class="hljs-number">396.451901</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.979203</span> Tokens per Sec: <span class="hljs-number">350.384045</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.648887</span> Tokens per Sec: <span class="hljs-number">361.534817</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.824539</span> Tokens per Sec: <span class="hljs-number">349.660287</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.550169</span> Tokens per Sec: <span class="hljs-number">319.302558</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.676636</span> Tokens per Sec: <span class="hljs-number">369.678638</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.394759</span> Tokens per Sec: <span class="hljs-number">364.660371</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.473153</span> Tokens per Sec: <span class="hljs-number">324.016068</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.142609</span> Tokens per Sec: <span class="hljs-number">422.345444</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.410883</span> Tokens per Sec: <span class="hljs-number">365.395922</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">0.828656</span> Tokens per Sec: <span class="hljs-number">401.538655</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">1.254409</span> Tokens per Sec: <span class="hljs-number">346.133228</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">0.745532</span> Tokens per Sec: <span class="hljs-number">402.395937</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">0.952969</span> Tokens per Sec: <span class="hljs-number">324.858870</span><br>Epoch Step: <span class="hljs-number">1</span> Loss: <span class="hljs-number">0.373509</span> Tokens per Sec: <span class="hljs-number">358.814760</span><br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>第四步: 使用模型进行贪婪解码</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入贪婪解码工具包greedy_decode, 该工具将对最终结进行贪婪解码</span><br><span class="hljs-comment"># 贪婪解码的方式是每次预测都选择概率最大的结果作为输出, </span><br><span class="hljs-comment"># 它不一定能获得全局最优性, 但却拥有最高的执行效率.</span><br><span class="hljs-keyword">from</span> pyitcast.transformer_utils <span class="hljs-keyword">import</span> greedy_decode <br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">model, loss, epochs=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-comment"># 首先进入训练模式，所有的参数将会被更新</span><br>        model.train()<br><br>        run_epoch(data_generator(V, <span class="hljs-number">8</span>, <span class="hljs-number">20</span>), model, loss)<br>		<br>        <span class="hljs-comment"># 训练结束后， 进入评估模式，所有的参数固定不变</span><br>        model.<span class="hljs-built_in">eval</span>()<br><br>        run_epoch(data_generator(V, <span class="hljs-number">8</span>, <span class="hljs-number">5</span>), model, loss)<br><br>    <span class="hljs-comment"># 跳出for循环后，掉膘模型训练结束，进入评估模式</span><br>    <span class="hljs-comment"># 模型进入测试模式</span><br>    model.<span class="hljs-built_in">eval</span>()<br><br>    <span class="hljs-comment"># 假定的输入张量</span><br>    <span class="hljs-comment"># 初始化一个输入张量</span><br>    source = Variable(torch.LongTensor([[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>]]))<br><br>    <span class="hljs-comment"># 定义源数据掩码张量, 因为元素都是1, 在我们这里1代表不遮掩</span><br>    <span class="hljs-comment"># 因此相当于对源数据没有任何遮掩.</span><br>    <span class="hljs-comment"># 初始化一个输入张量的掩码张量，全1代表没有任何的遮掩</span><br>    source_mask = Variable(torch.ones(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>))<br><br>    <span class="hljs-comment"># 最后将model, src, src_mask, 解码的最大长度限制max_len, 默认为10</span><br>    <span class="hljs-comment"># 以及起始标志数字, 默认为1, 我们这里使用的也是1</span><br>    <span class="hljs-comment">#设定解码的最大长度max_len等于10，起始数字的标志默认等于1</span><br><br>    result = greedy_decode(model, source, source_mask, max_len=<span class="hljs-number">10</span>, start_symbol=<span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(result)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    run(model, loss) <br></code></pre></td></tr></table></figure>

<hr>
<ul>
<li>输出效果:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">    <span class="hljs-number">1</span>     <span class="hljs-number">3</span>     <span class="hljs-number">2</span>     <span class="hljs-number">5</span>     <span class="hljs-number">4</span>     <span class="hljs-number">6</span>     <span class="hljs-number">7</span>     <span class="hljs-number">8</span>     <span class="hljs-number">9</span>    <span class="hljs-number">10</span><br>[torch.LongTensor of size 1x10]<br></code></pre></td></tr></table></figure>

<hr>
<p><strong>小节总结</strong></p>
<ul>
<li><p>学习了copy任务的相关知识:</p>
<ul>
<li>任务描述: 针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</li>
<li>任务意义: <code>copy任务</code>在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>学习了使用copy任务进行模型基本测试的四步曲:</p>
<ul>
<li>第一步: 构建数据集生成器</li>
<li>第二步: 获得Transformer模型及其优化器和损失函数</li>
<li>第三步: 运行模型进行训练和评估</li>
<li>第四步: 使用模型进行贪婪解码</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>学习并实现了构建数据集生成器函数: data_gen</p>
<ul>
<li>它有三个输入参数, 分别是V: 随机生成数字的最大值+1, batch: 每次输送给模型更新一次参数的数据量, nbatches: 一共输送nbatches次完成一轮.</li>
<li>该函数最终得到一个生成器对象.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>学习了获得Transformer模型及其优化器和损失函数:</p>
<ul>
<li>通过导入优化器工具包get_std_opt, 获得标准优化器.</li>
<li>通过导入标签平滑工具包LabelSmoothing, 进行标签平滑.</li>
<li>通过导入损失计算工具包SimpleLossCompute, 计算损失.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>学习并实现了运行模型进行训练和评估函数: run</p>
<ul>
<li>在函数中导入模型单轮训练工具包run_epoch, 对模型进行单轮训练.</li>
<li>函数共有三个参数, model代表将要进行训练的模型, slc代表使用的损失计算方法, epochs代表模型训练的轮数.</li>
<li>函数最终打印了模型训练和评估两个过程的损失.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>学习并实现了使用模型进行贪婪解码:</p>
<ul>
<li>通过导入贪婪解码工具包greedy_decode, 根据输入得到最后输出, 完成了copy任务.</li>
</ul>
</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%AD%A6%E4%B9%A0/" class="category-chain-item">学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Transformer/">#Transformer</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>第二章 Transformer</div>
      <div>http://example.com/2023/11/08/Transformer/Transformer第二章/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年11月8日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2023年11月8日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/11/09/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="环境 配置">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">环境 配置</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/11/08/Transformer/1.3/" title="1 3 Transformer">
                        <span class="hidden-mobile">1 3 Transformer</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  

<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
